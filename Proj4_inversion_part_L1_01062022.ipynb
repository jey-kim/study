{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figs/GEOS_logo.pdf\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert BC, InSAR and GNSS data (weighted and damped LSM): \n",
    "## <font color=blue>\"inversion_part.ipynb\"</font>\n",
    "#### Dec 31, 2021  <font color=red>(v. testing)</font>\n",
    "##### Jeonghyeop Kim (jeonghyeop.kim@gmail.com)\n",
    "\n",
    "1. This code is a part of the joint inversion project (project4: joint inversion of GNSS and InSAR)\n",
    "2. The G-matrix will be loaded from a file which was generated in the previous step\n",
    "3. Damped and Weighted LSM will be performed to find the best coefficients of the basis functions\n",
    "4. The goal is to find the best linear combination of the basis functions that predicted the data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########READ Weighting and Damping parameters\n",
    "\n",
    "# weight_for_InSAR = sys.argv[1]\n",
    "# weight_for_InSAR = float(weight_for_InSAR)\n",
    "\n",
    "# weight_for_GNSS = sys.argv[2]\n",
    "# weight_for_GNSS = float(weight_for_GNSS)\n",
    "# weight_for_BC = weight_for_GNSS \n",
    "# #BC velocity is from GNSS data\n",
    "\n",
    "# damping_for_horizontal = sys.argv[3]\n",
    "# damping_for_horizontal = float(damping_for_horizontal)\n",
    "# damping_for_rotation = damping_for_horizontal \n",
    "# # Rotations do NOT contribute the vertical field\n",
    "\n",
    "# damping_for_vertical = sys.argv[4]\n",
    "# damping_for_vertical = float(damping_for_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_InSAR=5\n",
    "weight_for_GNSS=1\n",
    "weight_for_BC = weight_for_GNSS\n",
    "damping_for_horizontal=0.01\n",
    "damping_for_rotation = damping_for_horizontal\n",
    "damping_for_vertical=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## How Many Basis Functions (FT #ofCells; BC #ofRotations)\n",
    "HowManyBasisFunctions=np.loadtxt(\"HowMany.txt\")\n",
    "HowManyCell=int(HowManyBasisFunctions[1])\n",
    "HowManyRot=int(HowManyBasisFunctions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmatrix_file=\"G_matrix.out\"\n",
    "df_G_final=pd.read_csv(Gmatrix_file, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CHOOSE THE TYPE OF LSMs for the solution\n",
    "\n",
    "inversion_flag = 4 \n",
    "\n",
    "# 1 is simple LSM\n",
    "# 2 is Pseudo LSM \n",
    "# 3 is Damped LSM (Ridge Regularization)\n",
    "\n",
    "################## Testing ###########################\n",
    "# 4 is Damped LSM (L-1 Norm : LASSO [scikit-learn])  #\n",
    "# 5 is Damped LSM (L-1 Norm : STLSQ )                #\n",
    "######################################################\n",
    "\n",
    "# If the inversion_flag = 4,\n",
    "# one damping parameter (alpah) will be used:\n",
    "# alpah=damping_for_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "outputFILE_GNSS_XandY=\"vel_GNSS_pred.gmt\" #Prediction for GNSS\n",
    "outputFILE_D=\"dsd_InSAR_pred.dat\" #Prediction for (Descending) InSAR\n",
    "outputFILE_A=\"asd_InSAR_pred.dat\" #Prediction for (Ascending) InSAR\n",
    "outputFILE_BC_XandY=\"vel_BC_pred.dat\" #Prediction for BC velocity field\n",
    "outputFILE_model=\"model_coef.dat\" # LSM model coefficients\n",
    "outputFILE_hori=\"vel_horizontal_cont_pred.gmt\" #Continuous horizontal field\n",
    "outputFILE_vert=\"vel_vertical_cont_pred.dat\" #Continuous vertical field \n",
    "outputFILE_strain=\"average_strain_cont_pred.out\" #Continuous Strain rate field (midpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `STEP 1:` **BUILD a data vector,  $\\vec{d}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input files\n",
    "\n",
    "# 1. Boundary velocity (on the boundary)\n",
    "inputBC = \"BC_input.gmt\"  # Boundary Condition Velocity [mm/yr]\n",
    "df_inputBC = pd.read_csv(inputBC, header = None, sep =' ')\n",
    "df_inputBC.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputBC.loc[:,['ve']] = df_inputBC.loc[:,['ve']]  \n",
    "df_inputBC.loc[:,['vn']] = df_inputBC.loc[:,['vn']]\n",
    "\n",
    "# 2. GNSS data\n",
    "inputGNSS = \"GNSS_input.gmt\"  # GNSS [mm/yr]\n",
    "df_inputGNSS = pd.read_csv(inputGNSS, header = None, sep=r'(?:,|\\s+)', comment='#', engine='python')\n",
    "df_inputGNSS.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputGNSS.loc[:,['ve']] = df_inputGNSS.loc[:,['ve']] \n",
    "df_inputGNSS.loc[:,['vn']] = df_inputGNSS.loc[:,['vn']]\n",
    "\n",
    "# 2. InSAR Descending \n",
    "inputInSAR_D = \"dsd_InSAR_input.dat\" # InSAR DECS [mm/yr]\n",
    "df_inputInSAR_D = pd.read_csv(inputInSAR_D, header = None, sep = ' ')\n",
    "df_inputInSAR_D.columns = ['lon','lat','velo','Px','Py','Pz']  \n",
    "df_inputInSAR_D.loc[:,['velo']] = df_inputInSAR_D.loc[:,['velo']]\n",
    "\n",
    "# 3. InSAR Ascending \n",
    "inputInSAR_A = \"asd_InSAR_input.dat\" # InSAR ASCE [mm/yr]\n",
    "df_inputInSAR_A = pd.read_csv(inputInSAR_A, header = None, sep = ' ')\n",
    "df_inputInSAR_A.columns = ['lon','lat','velo','Px','Py','Pz'] \n",
    "df_inputInSAR_A.loc[:,['velo']] = df_inputInSAR_A.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>NOTE: the pointing vectors are from the perspective of the ground! NOT of the satellite </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################         GNSS         ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD GNSS data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the GNSS velocity.\n",
    "df_data_x_GNSS = df_inputGNSS.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y_GNSS = df_inputGNSS.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "df_data_x_GNSS=df_data_x_GNSS.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y_GNSS=df_data_y_GNSS.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_data_x_GNSS=df_data_x_GNSS.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y_GNSS=df_data_y_GNSS.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesGNSS=[df_data_x_GNSS,df_data_y_GNSS]\n",
    "df_data_GNSS_all=pd.concat(framesGNSS,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "# SAVE GNSS velocity separately\n",
    "df_data_GNSS=df_data_GNSS_all.loc[:,['velo']]\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################       Boundary       ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD a boundary condition data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the velocity.\n",
    "df_data_x_BC = df_inputBC.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y_BC = df_inputBC.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "df_data_x_BC=df_data_x_BC.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y_BC=df_data_y_BC.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT_VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_data_x_BC=df_data_x_BC.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y_BC=df_data_y_BC.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesBC=[df_data_x_BC,df_data_y_BC]\n",
    "df_data_BC_all=pd.concat(framesBC,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "# SAVE BC velocity separately\n",
    "df_data_BC=df_data_BC_all.loc[:,['velo']]\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################        InSAR         ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD a InSAR data vector along with coordinate information.\n",
    "# The rows of the InSAR data vector is in the order of descending-orbit data, and ascending-orbit data. \n",
    "# Track the pointing vector values together with the rate data for the G-matrix.\n",
    "\n",
    "# !! SORT_VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_inputInSAR_D=df_inputInSAR_D.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR_A=df_inputInSAR_A.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesInSAR=[df_inputInSAR_D,df_inputInSAR_A]\n",
    "df_data_InSAR_all=pd.concat(framesInSAR,ignore_index=True) # merge the four dataFrames into one\n",
    "\n",
    "# SAVE InSAR velocity separately\n",
    "df_data_InSAR = df_data_InSAR_all.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GNSS, InSAR, and BC data vectors into the final data vector\n",
    "framesFinal = [df_data_GNSS, df_data_InSAR, df_data_BC]\n",
    "df_data_total = pd.concat(framesFinal,ignore_index=True) # merge the two dataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_total.columns = ['data'] # DATA VECTOR [[GNSSx], [GNSSy], [InSAR_D], [InSAR_A], [BCx], [BCy]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_data_total)!=len(df_G_final):\n",
    "    print(\"WARNING: Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 3:` RUN Joint Inversion (LSM)\n",
    "> G-matrix = **df_G_final** \\\n",
    "> data vec = **df_data_total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Diagonal Weighting Matrix W\n",
    "nGNSS=len(df_data_GNSS)\n",
    "nBC=len(df_data_BC)\n",
    "nInSAR=len(df_data_InSAR)\n",
    "nTotal=len(df_data_total)\n",
    "\n",
    "errorGNSS = np.ones(nGNSS)*weight_for_GNSS\n",
    "errorInSAR = np.ones(nInSAR)*weight_for_InSAR \n",
    "errorBC = np.ones(nBC)*weight_for_BC\n",
    "\n",
    "errorTotal = np.concatenate((errorGNSS,errorInSAR, errorBC),axis=0)\n",
    "errorTotalinv = 1/errorTotal\n",
    "W = np.diag(errorTotalinv)\n",
    "\n",
    "# convert into a dataframe\n",
    "dfW = pd.DataFrame(W)\n",
    "\n",
    "# When calculating predictions, the non-weighted G-matrix is needed. \n",
    "# SAVE it!\n",
    "df_G_final_save = df_G_final\n",
    "\n",
    "# When calculating the misfit, the non-weighted data is needed. \n",
    "# SAVE it!\n",
    "df_data_total_save = df_data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the Diagonal Weighting Matrix dfW to the data vector and Gmatrix\n",
    "\n",
    "df_G_final = dfW @ df_G_final_save\n",
    "df_data_total = dfW @ df_data_total_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_prime = df_G_final.transpose() \n",
    "\n",
    "# G'G\n",
    "# >Two different ways to compute a matrix multiplication\n",
    "# >1st method\n",
    "GpG1=df_G_prime.dot(df_G_final) #G'G\n",
    "# >2nd method\n",
    "GpG2=df_G_prime @ df_G_final #G'G\n",
    "# >These results are same.\n",
    "# >Let's take the second one as G'G\n",
    "GpG = GpG2 #GpG is G'G\n",
    "# inv(G'G)\n",
    "# > Two different ways to obtain inverse matrix\n",
    "# > 1st method: np.linalg.inv \n",
    "\n",
    "\n",
    "###############################\n",
    "## inv(G'G)*G'*d = model(LSM) #\n",
    "###############################\n",
    "if inversion_flag == 1:\n",
    "    #  1st method: np.linalg.inv\n",
    "    df_inv_GpG = pd.DataFrame(np.linalg.inv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_inv_GpG@df_G_prime@df_data_total #inversion\n",
    "    \n",
    "elif inversion_flag == 2:\n",
    "    #  2nd method: np.linalg.pinv (Moore-Penrose inverse (SVD))\n",
    "    df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion\n",
    "    \n",
    "elif inversion_flag == 3:\n",
    "    #  3rd method: Damping (Tikhonov Regularization)\n",
    "    alp=damping_for_horizontal*np.ones((HowManyCell*3,1))**2 # damping parameter for hori\n",
    "    bet=damping_for_vertical*np.ones((HowManyCell,1))**2 # damping parameter for vert\n",
    "    gam=damping_for_rotation*np.ones((HowManyRot*3,1))**2 # damping parameter for rot\n",
    "    array_tuple = (alp, bet, gam)\n",
    "    lamb = np.vstack(array_tuple) \n",
    "    lamb = lamb[:,0]\n",
    "    damping_matrix=np.diag(lamb) # a*a*I    \n",
    "    GpG_damping = GpG + damping_matrix #(G'G + a*a*I)\n",
    "    GpD=df_G_prime@df_data_total\n",
    "    df_model_damping= np.linalg.solve(GpG_damping,GpD)\n",
    "    df_model1=pd.DataFrame(df_model_damping[:,0])\n",
    "    df_model1.index=df_G_final_save.columns.values  \n",
    "\n",
    "    \n",
    "    \n",
    "elif inversion_flag == 4: \n",
    "#  4th method [Testing]: scikit-learn LASSO (L1-norm)\n",
    "\n",
    "    # For the Frame\n",
    "    df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion    \n",
    "    \n",
    "    \n",
    "    ## Cross Validation\n",
    "    # np_G_final = df_G_final.to_numpy()\n",
    "    # np_data_total= df_data_total.to_numpy()\n",
    "    # np_data_total = np.squeeze(np_data_total)\n",
    "    # from sklearn import linear_model\n",
    "    # from sklearn import model_selection\n",
    "    ## reg = linear_model.LassoCV(cv=10).fit(np_G_final, np_data_total)\n",
    "    # lasso = linear_model.Lasso(random_state=0, max_iter=10000)\n",
    "    # alphas = np.logspace(-4, -0.5, 30)\n",
    "    # tuned_parameters = [{'alpha': alphas}] \n",
    "    # clf = model_selection.GridSearchCV(lasso, tuned_parameters, cv=10, refit=False)\n",
    "    # clf.fit(np_G_final, np_data_total)\n",
    "    # XL1 = linear_model.Lasso(alpha=clf.best_params_['alpha'])\n",
    "    # XL1.fit(np_G_final, np_data_total)\n",
    "    # s_L1 = XL1.coef_\n",
    "    # scores = clf.cv_results_['mean_test_score']\n",
    "    # scores_std = clf.cv_results_['std_test_score']\n",
    "    # plt.semilogx(alphas, scores,'r-')\n",
    "    # # plot error lines showing +/- std. errors of the scores\n",
    "    # std_error = scores_std / np.sqrt(10)\n",
    "    # plt.semilogx(alphas, scores + std_error, 'k--')\n",
    "    # plt.semilogx(alphas, scores - std_error, 'k--')\n",
    "    # plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.1,color='k')\n",
    "    # plt.ylabel('CV score +/- std error')\n",
    "    # plt.xlabel('alpha')\n",
    "    # plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "    # plt.xlim([alphas[-1], alphas[0]])\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    np_G_final = df_G_final.to_numpy()\n",
    "    np_data_total= df_data_total.to_numpy()\n",
    "    np_data_total = np.squeeze(np_data_total)\n",
    "    from sklearn import linear_model  \n",
    "    clf = linear_model.Lasso(alpha=damping_for_horizontal)\n",
    "    clf.fit(np_G_final, np_data_total)\n",
    "    s_L1 = clf.coef_\n",
    "    a=s_L1.reshape(len(s_L1),1)\n",
    "    df_model1.loc[:,['data']]=a\n",
    "    ###### s_L1 to pandas Df named 'df_model1'\n",
    "    \n",
    "    \n",
    "else:\n",
    "    #  5th method [Testing]: STLSQ method (L1-norm)\n",
    "    # Started from L2-norm\n",
    "    alp=3*np.ones((HowManyCell*3,1))**2 # damping parameter for hori\n",
    "    bet=3*np.ones((HowManyCell,1))**2 # damping parameter for vert\n",
    "    gam=3*np.ones((HowManyRot*3,1))**2 # damping parameter for rot\n",
    "    array_tuple = (alp, bet, gam)\n",
    "    lamb = np.vstack(array_tuple) \n",
    "    lamb = lamb[:,0]\n",
    "    damping_matrix=np.diag(lamb) # a*a*I    \n",
    "    GpG_damping = GpG + damping_matrix #(G'G + a*a*I)\n",
    "    GpD=df_G_prime@df_data_total\n",
    "    df_model_damping= np.linalg.solve(GpG_damping,GpD)\n",
    "    df_model1=pd.DataFrame(df_model_damping[:,0])\n",
    "    df_model1.index=df_G_final_save.columns.values  \n",
    "\n",
    "    # STLSQ \n",
    "    x0 = df_model1.to_numpy() #initialize with L2 solution\n",
    "    from scipy.optimize import minimize\n",
    "    def L1_norm(x):\n",
    "        return np.linalg.norm(x,ord=1) \n",
    "    constr = ({'type' : 'eq', 'fun' : lambda x: df_G_final @ x - df_data_total})\n",
    "    res = minimize(L1_norm, x0, method = 'SLSQP', constraints = constr)\n",
    "    s_L1 = res.x\n",
    " \n",
    "    df_model1.loc[:,['data']]=s_L1\n",
    "    ###### s_L1 to pandas Df named 'df_model1'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_data_predicted = df_G_final_save @ df_model1\n",
    "# 'df_G_final_save' is the non-weighted G-matrix\n",
    "\n",
    "# squared norm2 misfit\n",
    "df_norm2=(df_data_total_save.to_numpy()-df_data_predicted.to_numpy())**2\n",
    "df_norm2=df_norm2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predicted data vector into\n",
    "# (1) GNSS x and y\n",
    "# (2) InSAR 1\n",
    "# (3) InSAR 2\n",
    "# (4) BC vel x and y\n",
    "\n",
    "num_velo_pointGNSS=len(df_data_GNSS_all) # GNSS\n",
    "num_velo_pointGNSS=int(num_velo_pointGNSS)\n",
    "\n",
    "num_velo_pointInSAR=len(df_data_InSAR_all)/2  # 2 InSAR data sets in a column vector\n",
    "num_velo_pointInSAR=int(num_velo_pointInSAR)\n",
    "\n",
    "num_velo_pointBC=len(df_data_BC_all) #BC vel\n",
    "num_velo_pointBC=int(num_velo_pointBC)\n",
    "\n",
    "\n",
    "####################\n",
    "#####  GNSS  #######\n",
    "####################\n",
    "\n",
    "#GNSS predicted x\n",
    "df_prediction_GNSS_x=df_data_predicted.iloc[0:int(num_velo_pointGNSS/2)] \n",
    "df_prediction_GNSS_x=df_prediction_GNSS_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_GNSS_y=df_data_predicted.iloc[int(num_velo_pointGNSS/2):num_velo_pointGNSS] \n",
    "df_prediction_GNSS_y=df_prediction_GNSS_y.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_GNSS_save = df_data_GNSS_all.iloc[0:int(num_velo_pointGNSS/2),[0,1]]\n",
    "\n",
    "df_save_GNSS_XandY = df_GNSS_save.reset_index(drop=True)\n",
    "df_save_GNSS_XandY['vx'] = df_prediction_GNSS_x\n",
    "df_save_GNSS_XandY['vn'] = df_prediction_GNSS_y\n",
    "df_save_GNSS_XandY['se'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['sn'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['corr'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#####  INSAR  #######\n",
    "#####################\n",
    "\n",
    "df_prediction_D=df_data_predicted.iloc[num_velo_pointGNSS:num_velo_pointGNSS+num_velo_pointInSAR] #dLOS Descending\n",
    "df_prediction_D=df_prediction_D.reset_index(drop=True)\n",
    "\n",
    "df_prediction_A=df_data_predicted.iloc[num_velo_pointGNSS+num_velo_pointInSAR:num_velo_pointGNSS+2*num_velo_pointInSAR] #dLOS Ascending\n",
    "df_prediction_A=df_prediction_A.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted InSAR Data\n",
    "df_InSAR_save = df_inputInSAR_D.iloc[:,[0,1]]   \n",
    "\n",
    "df_save_D = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_D['dLOS'] = df_prediction_D #append predicted dLOS Descending\n",
    "df_save_D['dLOS'] = df_save_D['dLOS']\n",
    "\n",
    "df_save_A = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A['dLOS'] = df_prediction_A #append predicted dLOS Ascending\n",
    "df_save_A['dLOS'] = df_save_A['dLOS']\n",
    "\n",
    "########################\n",
    "#####  Boundary ########\n",
    "########################\n",
    "\n",
    "#BC predicted x\n",
    "df_prediction_BC_x=df_data_predicted.iloc[num_velo_pointGNSS+2*num_velo_pointInSAR:num_velo_pointGNSS+2*num_velo_pointInSAR+int(num_velo_pointBC/2)] \n",
    "df_prediction_BC_x=df_prediction_BC_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_BC_y=df_data_predicted.iloc[num_velo_pointGNSS+2*num_velo_pointInSAR+int(num_velo_pointBC/2):num_velo_pointGNSS+2*num_velo_pointInSAR+num_velo_pointBC] \n",
    "df_prediction_BC_y=df_prediction_BC_y.reset_index(drop=True)\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_BC_save = df_data_BC_all.iloc[0:int(num_velo_pointBC/2),[0,1]]\n",
    "df_save_BC_XandY = df_BC_save.reset_index(drop=True)\n",
    "df_save_BC_XandY['vx'] = df_prediction_BC_x\n",
    "df_save_BC_XandY['vn'] = df_prediction_BC_y\n",
    "df_save_BC_XandY['se'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['sn'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['corr'] = np.zeros(len(df_prediction_BC_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE predicted data sets\n",
    "df_save_GNSS_XandY.to_csv(outputFILE_GNSS_XandY, header=None, index=None, sep=' ', float_format='%g')\n",
    "df_save_D.to_csv(outputFILE_D, header=None, index=None, sep=' ',float_format='%g')\n",
    "df_save_A.to_csv(outputFILE_A, header=None, index=None, sep=' ',float_format='%g')\n",
    "df_save_BC_XandY.to_csv(outputFILE_BC_XandY, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE model coefficients\n",
    "df_model1.to_csv(outputFILE_model, header=None, index=None, float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-square statistics is : 141573.161261 [mm/yr]**2\n"
     ]
    }
   ],
   "source": [
    "print(df_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<div class=\"alert--icon\"> <i class=\"far fa-times-circle\"></i> </div>\n",
    "    <p> Weighted Tikhonov Regularization works pretty well. </p>\n",
    "    <b> But this approach cannot resolve short-wavelength features. </b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> TRY the L1 regularization. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 4:` Obtain 3-D continuous surface velocity & horizontal strain field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmatrix_file_cont_hori=\"G_matrix_cont_hori.out\"\n",
    "Gmatrix_file_cont_vert=\"G_matrix_cont_vert.out\"\n",
    "Gmatrix_file_strain=\"G_matrix_strain.out\"\n",
    "\n",
    "df_G_horizontal_continuous=pd.read_csv(Gmatrix_file_cont_hori, sep = ',')\n",
    "df_G_vertical_continuous=pd.read_csv(Gmatrix_file_cont_vert, sep = ',')\n",
    "df_G_horizontal_continuous_strain=pd.read_csv(Gmatrix_file_strain, sep = ',')\n",
    "########################################################################################\n",
    "#                         Horizontal continuous velocity model.                        #\n",
    "########################################################################################\n",
    "\n",
    "continuous_hor_model = df_G_horizontal_continuous.to_numpy() @ df_model1.to_numpy()\n",
    "continuous_num=int(len(continuous_hor_model)/2)\n",
    "Xmodel=continuous_hor_model[0:continuous_num,0]\n",
    "Ymodel=continuous_hor_model[continuous_num:,0]\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#                         Vertical continuous velocity model.                          #\n",
    "########################################################################################\n",
    "\n",
    "continuous_ver_model = df_G_vertical_continuous.to_numpy() @ df_model1.to_numpy()[HowManyCell*3:HowManyCell*4,]\n",
    "Zmodel=continuous_ver_model[:,0]\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#                         Horizontal continuous strain rate model.                     #\n",
    "########################################################################################\n",
    "\n",
    "continuous_hor_model_strain = df_G_horizontal_continuous_strain.to_numpy() @ df_model1.to_numpy()\n",
    "midpointnum=int(len(continuous_hor_model_strain)/3)\n",
    "eXXmodel=continuous_hor_model_strain[0:midpointnum,0]\n",
    "eYYmodel=continuous_hor_model_strain[midpointnum:2*midpointnum,0]\n",
    "eXYmodel=continuous_hor_model_strain[2*midpointnum:3*midpointnum,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE continuous 3-D velocity field.\n",
    "inputfile_zzz_continuous = \"./basis_functions_FT/vel_vert_FT_1_4_continuous.gmt\"\n",
    "df_zzz_continuous=pd.read_csv(inputfile_zzz_continuous, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "df_zzz_continuous.columns = ['lon','lat','vz']\n",
    "df_zzz_continuous=df_zzz_continuous.sort_values(['lat','lon'], ascending=[True, True])\n",
    "df_zzz_continuous=df_zzz_continuous.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_hori = df_zzz_continuous.loc[:,['lon','lat']]\n",
    "df_hori['ve'] = Xmodel\n",
    "df_hori['vn'] = Ymodel\n",
    "df_hori['se'] = np.zeros(len(Ymodel))\n",
    "df_hori['sn'] = np.zeros(len(Ymodel))\n",
    "df_hori['corr'] = np.zeros(len(Ymodel))\n",
    "\n",
    "df_vert = df_zzz_continuous.loc[:,['lon','lat']]\n",
    "df_vert['vz'] = Zmodel\n",
    "\n",
    "df_hori.to_csv(outputFILE_hori, header=None, index=None, sep=' ', float_format='%g')\n",
    "df_vert.to_csv(outputFILE_vert, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE horizontal strain rate field.\n",
    "\n",
    "inputfile_strain_xrot = \"./basis_functions_BC/average_strain_BC_x_001_RECTANGULAR.out\" # x-rot \n",
    "df_xrot_strain=pd.read_csv(inputfile_strain_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "df_xrot_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "df_xrot_exx = df_xrot_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints for strain\n",
    "df_xrot_exx=df_xrot_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "df_xrot_exx=df_xrot_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "\n",
    "\n",
    "df_strain=df_xrot_exx.loc[:,['lon','lat']]\n",
    "df_strain['exx']=eXXmodel\n",
    "df_strain['eyy']=eYYmodel\n",
    "df_strain['exy']=eXYmodel\n",
    "df_strain['num']=range(len(eXXmodel))\n",
    "df_strain['sxx']=np.zeros((len(eXXmodel),))\n",
    "df_strain['syy']=np.zeros((len(eXXmodel),))\n",
    "df_strain['sxy']=np.zeros((len(eXXmodel),))\n",
    "df_strain_save = df_strain[['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']]\n",
    "\n",
    "df_strain_save.to_csv(outputFILE_strain, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GexxXPx+GexxYPy 1</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeyyXPx+GeyyYPy 1</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GexyXPx+GexyYPy 1</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GexxXPx+GexxYPy 2</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeyyXPx+GeyyYPy 2</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyrotXPx+GyrotYPy 75</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GzrotXPx+GzrotYPy 75</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GxrotXPx+GxrotYPy 76</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GyrotXPx+GyrotYPy 76</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GzrotXPx+GzrotYPy 76</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5988 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      data\n",
       "GexxXPx+GexxYPy 1     -0.0\n",
       "GeyyXPx+GeyyYPy 1     -0.0\n",
       "GexyXPx+GexyYPy 1     -0.0\n",
       "GexxXPx+GexxYPy 2     -0.0\n",
       "GeyyXPx+GeyyYPy 2     -0.0\n",
       "...                    ...\n",
       "GyrotXPx+GyrotYPy 75  -0.0\n",
       "GzrotXPx+GzrotYPy 75  -0.0\n",
       "GxrotXPx+GxrotYPy 76  -0.0\n",
       "GyrotXPx+GyrotYPy 76  -0.0\n",
       "GzrotXPx+GzrotYPy 76  -0.0\n",
       "\n",
       "[5988 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/zdsy5jqj2tn2lbh6qhjxdlgh0000gn/T/ipykernel_2203/2214478011.py:3: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  ax.set_xticks([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_model1,'*')\n",
    "ax.set_xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
