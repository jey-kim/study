{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff455dc-5a84-4c8e-b000-60db30d360a7",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214ab56-a84b-40fb-8302-40c419ba8dc4",
   "metadata": {},
   "source": [
    "## 1. PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f56162-4dc3-44ee-95cc-6f705081195c",
   "metadata": {},
   "source": [
    "## 2. FEATURE ENGINEERING & SELECTION based on EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08f25a-b95a-4b22-9ba3-580ce27d4373",
   "metadata": {},
   "source": [
    "## 3. LightGBM \n",
    "\n",
    "\n",
    "#### Learn the concepts first\n",
    "\n",
    "- Gradient Boosting [GBM](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n",
    "- XGBOOST [XGB](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "- LightGBM [LGBM](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n",
    "\n",
    ">> LGB + Bayesian Optimization:  https://www.kaggle.com/code/vincentlugat/ieee-lgb-bayesian-opt/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f01550-9c0c-47fc-9eb2-bfd305ee1b50",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb1740-a732-4cb5-a550-2761b977b192",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fdb21-a008-4c18-bf22-19776d36d817",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74426ad-d440-4ae8-aac2-cdfdf5cf23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "import gc\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0e4cf1-b222-4efb-8f7b-5e7a6cd1dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Reduce memory\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \n",
    "    \"\"\" Iterate through all the columns of a dataframe and \n",
    "        modify the data type to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    # df.memory_usage() : bytes for each columns\n",
    "    # / 1024*1024 : bytes to Megabytes\n",
    "    \n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # int64 or float64\n",
    "        if col_type != object: \n",
    "            \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # int64\n",
    "            if str(col_type)[:3] == 'int': \n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    # e.g., np.iinfo(np.int16).max = 32767\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int36).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "                    \n",
    "            # float64\n",
    "            else: \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "        # string       \n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "            \n",
    "        \n",
    "    end_mem = df.memory_usage().sum() / 1024**2    \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem-end_mem)/start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5525abe-0d29-4e00-8fff-f77c68c0e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tSuccessfully loaded train_identity!\n",
      "\tSuccessfully loaded train_transaction!\n",
      "\tSuccessfully loaded test_identity!\n",
      "\tSuccessfully loaded test_transaction!\n",
      "\tSuccessfully loaded sample_submission!\n",
      "Data was successfully loaded!\n",
      "\n",
      "CPU times: user 11.1 s, sys: 1.54 s, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 1.3 Load data sets\n",
    "\n",
    "dir_path = '/Users/jkim/main/kaggle/ieee_cis_fraud/'\n",
    "print('Loading data...')\n",
    "\n",
    "train_identity = pd.read_csv(f'{dir_path}train_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_identity!')\n",
    "\n",
    "train_transaction = pd.read_csv(f'{dir_path}train_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_transaction!')\n",
    "\n",
    "test_identity = pd.read_csv(f'{dir_path}test_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_identity!')\n",
    "\n",
    "test_transaction = pd.read_csv(f'{dir_path}test_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_transaction!')\n",
    "\n",
    "sub = pd.read_csv(f'{dir_path}sample_submission.csv')\n",
    "print('\\tSuccessfully loaded sample_submission!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f907e4-ebc8-48d9-bf76-064cf0e57595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08',\n",
       "       'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n",
       "       'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24',\n",
       "       'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n",
       "       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n",
       "       'DeviceInfo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_identity.columns = test_identity.columns.str.replace('-','_')\n",
    "test_identity.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0949a9-42c9-4447-8137-ea364c8581f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4 preprocessing for identity data\n",
    "\n",
    "def id_split(df):\n",
    "    \n",
    "    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "    \n",
    "    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "    \n",
    "    \n",
    "    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    df['had_id'] = 1\n",
    "    \n",
    "    \n",
    "    ## id_23 ':'\n",
    "    df['id_23'] = df['id_23'].str.split(':', expand=True)[1]\n",
    "    ## id_30 'OS  Version'\n",
    "    df['OS_id_30'] = df['id_30'].str.split(' ',expand=True)[0]\n",
    "    df['version_id_30'] = df['id_30'].str.split(' ',expand=True)[1]\n",
    "    \n",
    "    ## id_31 'Browser' 'Version'\n",
    "    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n",
    "    df[(df['browser_id_31']=='mobile')] == 'safari'\n",
    "    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]\n",
    "    \n",
    "    ## id_33 'x' [e.g. 2220x1080]\n",
    "    df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n",
    "    df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n",
    "    \n",
    "    ## id_34 ':'\n",
    "    df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n",
    "    \n",
    "    ## Number of nans\n",
    "    df['nulls'] = df.isnull().sum(axis=1)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e800e16a-af44-4ede-b54f-9b78d766a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = id_split(train_identity)\n",
    "test_identity = id_split(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac869995-e44b-4f0b-9a2c-0558a4161970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Data was successfully merged!\n",
      "\n",
      "Train dataset has 590540 rows and 443 columns.\n",
      "Test dataset has 506691 rows and 442 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print('Data was successfully merged!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537f1892-43cf-4631-ae90-e5f4ff265e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Based on nroman's recursive feature elimination\n",
    "\n",
    "useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n",
    "                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n",
    "                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n",
    "                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n",
    "                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n",
    "                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n",
    "                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n",
    "                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n",
    "                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n",
    "                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n",
    "                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n",
    "                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n",
    "                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n",
    "                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n",
    "                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n",
    "                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n",
    "                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n",
    "                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n",
    "                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n",
    "                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n",
    "                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id','nulls']\n",
    "len(useful_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26c4118-0322-4cf8-8f2a-3e7b81f1725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in train.columns if col not in useful_features]\n",
    "cols_to_drop.remove('isFraud')  # remove(element) removes the first matching element in a list. If there is no element, error will occur\n",
    "cols_to_drop.remove('TransactionDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14eee947-805a-476a-9a85-c3617b564356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 features are going to be dropped for being useless\n"
     ]
    }
   ],
   "source": [
    "print('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n",
    "train = train.drop(cols_to_drop, axis=1) #list=cols_to_drop ;axis = 1 meaning columns\n",
    "test = test.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fefe0b5-7331-419f-8e7d-8777b9b98b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New feature - decimal part of the transaction amount\n",
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "# Count encoding for card1 feature. \n",
    "# Explained in this kernel: https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n",
    "train['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n",
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n",
    "\n",
    "# Some arbitrary features interaction\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "    \n",
    "for feature in ['id_34', 'id_36']:\n",
    "    if feature in useful_features:\n",
    "        # Count encoded for both train and test\n",
    "        train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "        test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "        \n",
    "for feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n",
    "    if feature in useful_features:\n",
    "        # Count encoded separately for train and test\n",
    "        train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n",
    "        test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb02765-5a8a-4405-a9eb-ddafdbbd52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### decimal amount can tell if the amount is integer. \n",
    "#### But my analysis shows that the transaction amounts that are multiple of 10 or 5 have higher fraud rates than those not multiple of 5 or 10. \n",
    "\n",
    "def add_multiple_features(df,multiple,minAmt):\n",
    "    \"\"\"\n",
    "    use 1 or 0\n",
    "    \"\"\"\n",
    "    multiple_str = str(multiple)\n",
    "    minAmt_str = str(minAmt)\n",
    "    isMultiple = (df.TransactionAmt.values % multiple == 0 ) & (df.TransactionAmt.values >= minAmt )\n",
    "    isMultiple = str(isMultiple)\n",
    "    df['multiple_' + multiple_str + '_' + minAmt_str] = isMultiple\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bbdfc1-cd1c-46e9-9034-96a64fd8e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=add_multiple_features(train,10,0)\n",
    "train=add_multiple_features(train,10,100)\n",
    "train=add_multiple_features(train,10,200)\n",
    "train=add_multiple_features(train,5,0)\n",
    "train=add_multiple_features(train,5,100)\n",
    "train=add_multiple_features(train,5,200)\n",
    "\n",
    "\n",
    "test=add_multiple_features(test,10,0)\n",
    "test=add_multiple_features(test,10,100)\n",
    "test=add_multiple_features(test,10,200)\n",
    "test=add_multiple_features(test,5,0)\n",
    "test=add_multiple_features(test,5,100)\n",
    "test=add_multiple_features(test,5,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b312073b-87d3-497f-80f6-6538d3d9fe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88be949d104c41849ebc88c275cb54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## tqdm_notebook is a progressbar decorator using the ipython widget. \n",
    "## sklearn.preprocessing.LabelEncoder : Encode target labels with value between 0 and n_classes-1.\n",
    "## fit means mapping a label column; assign a unique values to each label.\n",
    "## transform means transform using the map\n",
    "\n",
    "for col in tqdm_notebook(train.columns):  \n",
    "    if train[col].dtype == 'object':\n",
    "        #print(f'Names: {col}')\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values)) \n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc880574-bb23-43eb-8f7b-2b6824509066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud']\n",
    "test = test.sort_values('TransactionDT').drop(['TransactionDT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec48e86-83c4-46f7-9e85-70bbcc3f7144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 314), (506691, 314))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()\n",
    "X.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557deb20-9db3-4936-988e-8174e63b83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bec2cb47-1d09-4280-83cf-661e6e27493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "aucs = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n",
    "    start_time = time()\n",
    "    print('Training on fold {}'.format(fold + 1))\n",
    "    \n",
    "    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n",
    "    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n",
    "    aucs.append(clf.best_score['valid_1']['auc'])\n",
    "    \n",
    "    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('Mean AUC:', np.mean(aucs))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f69e77-5de9-41e4-bf6b-4d93aad6bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('feature_importances.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718a64d-332e-4c6e-9f4a-4fbdabfc1be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
