{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with kernel machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use simple two-dimensional data sets to illustrate the behavior of the support vector machine and the Perceptron, when used with quadratic and RBF kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory containing this notebook should also contain two-dimensional data files, `data1.txt` through `data5.txt`. These files contain one data point per line, along with a label (either -1 or 1), like:\n",
    "* `3 8 -1` (meaning that point `x=(3,8)` has label `y=-1`)\n",
    "\n",
    "The next procedure, **learn_and_display_SVM**, loads one of these data sets, invokes `sklearn.SVC` to learn a classifier, and then displays the data as well as the boundary. It is invoked as follows:\n",
    "* `learn_and_display_SVM(datafile, kernel_type, C_value, s_value)`\n",
    " \n",
    "**`NOTE: the C value is for the soft margin SVM method and the s value is for the RBF Kernel`**\n",
    "\n",
    "where\n",
    "* `datafile` is one of `'data1.txt'` through `'data5.txt'` (or another file in the same format)\n",
    "* `kernel_type` is either `'quadratic'` or `'rbf'`\n",
    "* `C_value` is the setting of the soft-margin parameter `C` (default: 1.0)\n",
    "* `s_value` (for the RBF kernel) is the scaling parameter `s` (default: 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_and_display_SVM(datafile, kernel_type='rbf', C_value=1.0, s_value=1.0):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n,d = data.shape\n",
    "    # Create training set x and labels y\n",
    "    x = data[:,0:2]\n",
    "    y = data[:,2]\n",
    "    # Now train a support vector machine and identify the support vectors\n",
    "    if kernel_type == 'rbf':\n",
    "        clf = SVC(kernel='rbf', C=C_value, gamma=1.0/(s_value*s_value))\n",
    "    if kernel_type == 'quadratic':\n",
    "        clf = SVC(kernel='poly', degree=2, C=C_value, coef0=1.0)\n",
    "    clf.fit(x,y) #clf = classifier\n",
    "    sv = np.zeros(n,dtype=bool) #0 == False \n",
    "    sv[clf.support_] = True \n",
    "    save_this = clf.support_  # 'clf.support_' gives the indice of the support vectors\n",
    "    notsv = np.logical_not(sv) \n",
    "    # False in 'sv' will be True in 'notsv' and vice and versa\n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    x1min = min(x[:,0]) - 1\n",
    "    x1max = max(x[:,0]) + 1\n",
    "    x2min = min(x[:,1]) - 1\n",
    "    x2max = max(x[:,1]) + 1\n",
    "    plt.xlim(x1min,x1max)\n",
    "    plt.ylim(x2min,x2max)\n",
    "    # Plot the data points, enlarging those that are support vectors\n",
    "    plt.plot(x[(y==1)*notsv,0], x[(y==1)*notsv,1], 'ro')  #y==1 & notsv ==> red + smaller\n",
    "    plt.plot(x[(y==1)*sv,0], x[(y==1)*sv,1], 'ro', markersize=10) #y==1 & sv ==> red + bigger\n",
    "    plt.plot(x[(y==-1)*notsv,0], x[(y==-1)*notsv,1], 'k^')\n",
    "    plt.plot(x[(y==-1)*sv,0], x[(y==-1)*sv,1], 'k^', markersize=10)\n",
    "    # Construct a grid of points and evaluate classifier at each grid points\n",
    "    grid_spacing = 0.05\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max, grid_spacing), np.arange(x2min, x2max, grid_spacing))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    Z = clf.decision_function(grid)\n",
    "    # Quantize the values to -1, -0.5, 0, 0.5, 1 for display purposes\n",
    "    for i in range(len(Z)):\n",
    "        Z[i] = min(Z[i],1.0)\n",
    "        Z[i] = max(Z[i],-1.0)\n",
    "        if (Z[i] > 0.0) and (Z[i] < 1.0):\n",
    "            Z[i] = 0.5\n",
    "        if (Z[i] < 0.0) and (Z[i] > -1.0):\n",
    "            Z[i] = -0.5\n",
    "    # Show boundary and margin using a color plot\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2)\n",
    "    plt.show()\n",
    "    \n",
    "    #return save_this, x,y,notsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments with the quadratic kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out SVM on some examples, starting with the quadratic kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data1.txt', 'quadratic', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data1.txt', 'quadratic', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data1.txt', 'quadratic', 30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try `data2.txt` through `data5.txt`. Also try changing the value of `C` (the third parameter) to see how that affects the boundary and margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data2.txt', 'quadratic', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data3.txt', 'quadratic', 30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data4.txt', 'quadratic', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data4.txt', 'quadratic', 10000000000000.0) \n",
    "# a little bit of slack costs a lot in the minimization. Thus, it is like a hard-margin SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data1.txt', 'quadratic', 1000000.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments with the RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now experiment with the RBF kernel, on the same five data sets. This time there are two parameters to play with: `C` and `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_and_display_SVM('data1.txt', 'rbf', 10.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The kernel Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do:**</font> Implement the kernel Perceptron algorithm as specified in lecture. Your algorithm should allow both the quadratic and RBF kernel, and should follow roughly the same signature as the SVM routine above:\n",
    "* `learn_and_display_Perceptron(datafile, kernel_type, s_value)`\n",
    "\n",
    "Recall that the Perceptron algorithm does not always converge; you will need to explicitly check for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "**Radial Basis Function (RBF) Kernel** : \n",
    "    k(x,z) = exp(-1*(Euclidean_dist^2)/s^2)\n",
    "    \n",
    "**Quadratic Kernel** :\n",
    "    k(x,z) = (1+x*z)^2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=30\n",
    "# x_picked=x[i]\n",
    "# y_picked=y[i]\n",
    "# n=len(x)\n",
    "# alpah = np.zeros(n)\n",
    "# b = 0\n",
    "# s = 10\n",
    "# kernel_type = \"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(x,y,x_picked,y_picked,alpah,b,s,kernel_type):\n",
    "    sum_final = 0\n",
    "    if kernel_type =='quadratic':\n",
    "        print('use quadratic kernel')\n",
    "        for num_x in range(len(x)):\n",
    "            sum_for = alpah[num_x]*y[num_x]*(np.dot(x[num_x],x_picked)+1)**2\n",
    "            sum_final = sum_final + sum_for\n",
    "        if y_picked*(sum_final+b) <= 0:\n",
    "            results = -1   \n",
    "        if y_picked*(sum_final+b) > 0:\n",
    "            results = 1\n",
    "            \n",
    "    if kernel_type =='rbf':\n",
    "        print('use RBF kernel')\n",
    "        for num_x in range(len(x)):\n",
    "            sum_for = alpah[num_x]*y[num_x]*np.exp(-1*((x[num_x,0]-x_picked[0])**2+(x[num_x,1]-x_picked[1])**2)/s**2)\n",
    "            sum_final = sum_final + sum_for\n",
    "        if y_picked*(sum_final+b) <= 0:\n",
    "            results = -1\n",
    "        if y_picked*(sum_final+b) > 0:\n",
    "            results = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(x,y,s,n_iters=100):\n",
    "    n=len(x)\n",
    "    alpah = np.zeros(n)\n",
    "    b = 0 \n",
    "    done = False\n",
    "    converged = True\n",
    "    iters = 0\n",
    "    #kernel_type='rbf'\n",
    "    np.random.seed(None)\n",
    "    while not(done):\n",
    "        done = True\n",
    "        I = np.random.permutation(n)\n",
    "        for k in range(n):\n",
    "            i = I[k]\n",
    "            x_picked=x[i]\n",
    "            y_picked=y[i]\n",
    "            results=evaluate_classifier(x,y,x_picked,y_picked,alpah,b,s,kernel_type)\n",
    "            if results == -1: \n",
    "                alpah[i]=alpah[i] + 1\n",
    "                b = b + y[i]\n",
    "                done = False\n",
    "        iters = iters + 1\n",
    "        \n",
    "        if iters > n_iters:\n",
    "            done = True\n",
    "            converged = False\n",
    "            \n",
    "    if converged:\n",
    "        print(\"Perceptron algorithm: iterations until convergence: \", iters)\n",
    "    else:\n",
    "        print(\"Perceptron algorithm: did not converge within the specified number of iterations\")\n",
    "    return alpah, b, converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_and_display_Perceptron(datafile, kernel_type, s_value=1.0):\n",
    "    \n",
    "    data = np.loadtxt(datafile)\n",
    "    n,d = data.shape\n",
    "    x = data[:,0:2]\n",
    "    y = data[:,2]\n",
    "    s = s_value\n",
    "    alpah,b,converged=train_perceptron(x,y,s,n_iters=100)\n",
    "    # trained parameters\n",
    "    \n",
    "    #\n",
    "    x1min = min(x[:,0]) - 1\n",
    "    x1max = max(x[:,0]) + 1\n",
    "    x2min = min(x[:,1]) - 1\n",
    "    x2max = max(x[:,1]) + 1\n",
    "    \n",
    "    plt.xlim(x1min,x1max)\n",
    "    plt.ylim(x2min,x2max)\n",
    "    \n",
    "    plt.plot(x[(y==1),0], x[(y==1),1], 'ro', markersize=10)\n",
    "    plt.plot(x[(y==-1),0], x[(y==-1),1], 'k^', markersize=10)\n",
    "    \n",
    "    grid_spacing = 0.05\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max, grid_spacing), np.arange(x2min, x2max, grid_spacing))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return alpah,b,converged,grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kernel_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e403f55f8758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpah\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_and_display_Perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data5.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'quadratic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6dffc79716e5>\u001b[0m in \u001b[0;36mlearn_and_display_Perceptron\u001b[0;34m(datafile, kernel_type, s_value)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0malpah\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# trained parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-718a91b9bfa6>\u001b[0m in \u001b[0;36mtrain_perceptron\u001b[0;34m(x, y, s, n_iters)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx_picked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0my_picked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_picked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_picked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpah\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0malpah\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpah\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kernel_type' is not defined"
     ]
    }
   ],
   "source": [
    "alpah,b,converged,grid=learn_and_display_Perceptron('data5.txt','quadratic', s_value=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fix the error : 'kernel_type' variable is not properly loaded. \n",
    "\n",
    "2. plot the prediction using the 'grid' test datasets for the boundary \n",
    ">    (np.dot(x[num_x],x_picked)+1)**2 :quadratic kernel\n",
    ">    np.exp(-1*((x[num_x,0]-x_picked[0])**2+(x[num_x,1]-x_picked[1])**2)/s**2) :RBF kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">Experiment with your routine, on the same five data sets.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
