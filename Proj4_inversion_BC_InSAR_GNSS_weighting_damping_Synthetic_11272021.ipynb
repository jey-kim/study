{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"Figs/GEOS_logo.pdf\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert BC, InSAR and GNSS data (weighted and damping LSM): <font color=blue>\"inversion_BC_InSAR_GNSS_weighting_damping.ipynb\"</font>\n",
    "#### Nov 27, 2021  <font color=red>(v. testing)</font>\n",
    "##### Jeonghyeop Kim (jeonghyeop.kim@gmail.com)\n",
    "\n",
    "\n",
    ">  input files :  \\\n",
    "> <font color=red> **Basis functions related to force terms** </font> \\\n",
    "> **`vel_hori_FT_i_j_on_InSAR.gmt`**, **`vel_hori_FT_i_j_on_boundary.gmt`**, **`vel_hori_FT_i_j_on_GNSS.gmt`** \\\n",
    "> **`vel_vert_FT_i_j_on_InSAR.gmt`**, **`vel_vert_FT_i_j_on_boundary.gmt`**, **`vel_vert_FT_i_j_on_GNSS.gmt`** \\\n",
    "> **`<i = 1..360 & j = 1..4>`** \\\n",
    "> <font color=red> **Basis functions related to boundary conditions** </font> \\\n",
    "**`vel_BC_k_l_on_boundary.gmt`**, **`vel_BC_k_l_on_InSAR.gmt`**, **`vel_BC_k_l_on_GNSS.gmt`** \\\n",
    "> **`<k = x, y, z & l = 001..038>`** \\\n",
    "> <font color=red> **Data vector made of GNSS (x and y), 2 InSAR data sets + Boundary velocities** </font> \\\n",
    "> **`DT42_data_new_ref_mm.dat`** and **`AT35_data_new_ref_mm.dat`** \\\n",
    "> **`<lon lat rate (mm/yr) Px Py Pz>`** \\\n",
    "> **`vel_GNSS_rotated.gmt`** : GNSS data (in mm/yr) \\\n",
    "> **`vel_BC_GNSS_ref_no_refpoint.gmt`** : B.C. velocities (in mm/yr) \\\n",
    ">\n",
    "> <font color=red> **For a continuous field (3-D velocity and strain rates), load the following files:** </font> \\\n",
    "> **`vel_#####_on_knotpoints`** & **`average_strain_#####_RECTENGULAR.out`**\n",
    ">\n",
    "> output files : \\\n",
    "> <font color=red> **predicted data** </font> \\\n",
    "> **`DT42_data_new_ref_pred.dat`** , **`AT35_data_new_ref_pred.dat`** \\\n",
    "> **`vel_GNSS_rotated_pred.gmt`** \\\n",
    "> **`vel_BC_pred.dat`** \\\n",
    "> **`vel_horizontal_cont_pred.gmt`** & **`vel_vertical_cont_pred.dat`** \\\n",
    "> **`average_strain_cont_pred.out`** \\\n",
    "0. This code is a part of the joint inversion project (project4: joint inversion of GNSS and InSAR)\n",
    "1. This code uses two types of basis functions generated in the previous steps : BC and FT \n",
    "2. This code jointly invert 4 InSAR data and 1 boundary velocity data for coefficients of each basis function\n",
    "> (a) d=Gm (d=data; G=basis functions; m=model coefficients) \\\n",
    "> (b) m(best) = inv(G'G)G'd \\\n",
    "> (c) m(best) * continuous surface 3-D velocities (from basis functions) \n",
    "3. Try using different weighting factors for BC and GNSS data, and InSAR data.\n",
    "> BC velocities and GNSS data have smaller error than InSAR data in general. \\\n",
    "> Apply the weighting LSM to the final step only. \n",
    "4. All \"magic\" comments were removed due to errors when a converted script is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # If you use jupyter notebook, \n",
    "# # # # comment out this cell and use the cell below instead. \n",
    "\n",
    "# weight_for_BC = sys.argv[1]\n",
    "# weight_for_BC = float(weight_for_BC)\n",
    "\n",
    "# weight_for_GNSS = weight_for_BC\n",
    "\n",
    "# weight_for_InSAR = sys.argv[2]\n",
    "# weight_for_InSAR = float(weight_for_InSAR)\n",
    "\n",
    "# damping_for_horizontal = sys.argv[3]\n",
    "# damping_for_horizontal = float(damping_for_horizontal)\n",
    "\n",
    "# damping_for_rotation = damping_for_horizontal\n",
    "\n",
    "# damping_for_vertical = sys.argv[4]\n",
    "# damping_for_vertical = float(damping_for_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you use the converted python script,\n",
    "# # comment out this cell and use the cell above instead.\n",
    "weight_for_BC = 1\n",
    "weight_for_GNSS = weight_for_BC\n",
    "weight_for_InSAR = 1 # This weighting factor will be inversely taken e.g., 8 => 1/8\n",
    "\n",
    "\n",
    "damping_for_horizontal = 1\n",
    "damping_for_rotation = damping_for_horizontal\n",
    "damping_for_vertical = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inversion types\n",
    "\n",
    "#inversion_flag = 1 # Simple LSM\n",
    "#inversion_flag = 2 # Pseudo LSM \n",
    "inversion_flag = 3 # Damped LSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP1`: **BUILD a data vector,  $\\vec{d}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input files\n",
    "\n",
    "# 1. Boundary velocity (76 data points on the boundary)\n",
    "inputBC = \"vel_BC_GNSS_ref_no_refpoint.gmt\"  #velocity boundary condition\n",
    "df_inputBC = pd.read_csv(inputBC, header = None, sep =' ')\n",
    "df_inputBC.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputBC.loc[:,['ve']] = df_inputBC.loc[:,['ve']]  \n",
    "df_inputBC.loc[:,['vn']] = df_inputBC.loc[:,['vn']]\n",
    "\n",
    "# 2. GNSS data\n",
    "inputGNSS = \"GNSS_horizontal_synthetic.gmt\"  # GNSS\n",
    "df_inputGNSS = pd.read_csv(inputGNSS, header = None, sep=r'(?:,|\\s+)', comment='#', engine='python')\n",
    "df_inputGNSS.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputGNSS.loc[:,['ve']] = df_inputGNSS.loc[:,['ve']] \n",
    "df_inputGNSS.loc[:,['vn']] = df_inputGNSS.loc[:,['vn']]\n",
    "\n",
    "# 2. InSAR Descending \n",
    "inputInSAR1 = \"DT173_data.dat\"\n",
    "df_inputInSAR1 = pd.read_csv(inputInSAR1, header = None, sep = ' ')\n",
    "df_inputInSAR1.columns = ['lon','lat','velo','Px','Py','Pz']  \n",
    "df_inputInSAR1.loc[:,['velo']] = df_inputInSAR1.loc[:,['velo']]\n",
    "\n",
    "# 3. InSAR Ascending \n",
    "inputInSAR2 = \"AT64_data.dat\"\n",
    "df_inputInSAR2 = pd.read_csv(inputInSAR2, header = None, sep = ' ')\n",
    "df_inputInSAR2.columns = ['lon','lat','velo','Px','Py','Pz'] \n",
    "df_inputInSAR2.loc[:,['velo']] = df_inputInSAR2.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>NOTE: the pointing vectors are from the perspective of the ground! NOT of the satellite </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a boundary condition data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the velocity.\n",
    "\n",
    "df_data_x = df_inputGNSS.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y = df_inputGNSS.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "\n",
    "df_data_x=df_data_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y=df_data_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "\n",
    "df_data_x=df_data_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y=df_data_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*1) into a new column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "\n",
    "framesGNSS=[df_data_x,df_data_y]\n",
    "df_data_GNSS_all=pd.concat(framesGNSS,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "df_data_GNSS=df_data_GNSS_all.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a boundary condition data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the velocity.\n",
    "\n",
    "df_data_x = df_inputBC.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y = df_inputBC.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "\n",
    "df_data_x=df_data_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y=df_data_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "\n",
    "df_data_x=df_data_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y=df_data_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*1) into a new column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "\n",
    "framesBC=[df_data_x,df_data_y]\n",
    "df_data_BC_all=pd.concat(framesBC,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "df_data_BC=df_data_BC_all.loc[:,['velo']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a InSAR data vector along with coordinate information.\n",
    "# The rows of the InSAR data vector is in the order of Ascending 1, Asceding 2, Descending 1, and Descending 2. \n",
    "# Track the pointing vector values together with the rate data for the G-matrix.\n",
    "\n",
    "df_inputInSAR1=df_inputInSAR1.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR2=df_inputInSAR2.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "\n",
    "framesInSAR=[df_inputInSAR1,df_inputInSAR2]\n",
    "df_data_InSAR_all=pd.concat(framesInSAR,ignore_index=True) # merge the four dataFrames into one\n",
    "df_data_InSAR = df_data_InSAR_all.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GNSS, InSAR, and BC data vectors into the final data vector\n",
    "\n",
    "framesFinal = [df_data_GNSS, df_data_InSAR, df_data_BC]\n",
    "df_data_total = pd.concat(framesFinal,ignore_index=True) # merge the two dataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_total.columns = ['data'] # DATA VECTOR [GNSSx; GNSSy; InSAR1; InSAR2; BCx; BCy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:   \n",
    ">If your data have errors, \\\n",
    "build a **W** matrix in which its diagonal elements \\\n",
    "are of a vector made of 1/errors (in order).  \\\n",
    "Your new data vector **d<sub>w</sub>** = **W** **d**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP2`: **BUILD G-Matrix, $\\bar{\\bar{G}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on the read_csv(sep option) below: \\\n",
    "> **`'(?:,|\\s+)'`** - is a RegEx for selecting either comma or any number of consecutive spaces/tabs \\\n",
    "> See the answer at StackOverFlow [click here](https://stackoverflow.com/questions/43784422/pandas-error-when-reading-csv-file-using-sep-and-comment-arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-a : Build G matrix part only related to Boundary Condition on GNSS data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_BC_on_GNSS = pd.DataFrame(index = range(len(df_data_GNSS))) \n",
    "# Make a blank G matrix part related to Boundary Condition on GNSS points\n",
    "\n",
    "# print('How many files do you have for the boudnary basis functions? :')\n",
    "# HowMany = input()\n",
    "# HowMany = int(HowMany)\n",
    "#38 for the boundary condition (11/17/2021)\n",
    "HowMany=38\n",
    "# print(\"38 (fixed for now [11/17/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_GNSS.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_GNSS.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_GNSS.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_xrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_yrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gx (i)\n",
    "\n",
    "    df_xrot_x = df_xrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_xrot_y = df_xrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_xrot_x=df_xrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_xrot_y=df_xrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_xrot_x['flag']=np.array([1] * len(df_xrot_x)) # 1 = velocity east-component\n",
    "#     df_xrot_y['flag']=np.array([2] * len(df_xrot_y)) # 2 = velocity north-component\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_x=df_xrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_y=df_xrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx=[df_xrot_x,df_xrot_y]\n",
    "    df_Gx=pd.concat(frames_Gx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "\n",
    "    df_yrot_x = df_yrot.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_yrot_y = df_yrot.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_yrot_x=df_yrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_yrot_y=df_yrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_yrot_x['flag']=np.array([1] * len(df_yrot_x)) # 1 = velocity east-component\n",
    "#     df_yrot_y['flag']=np.array([2] * len(df_yrot_y)) # 2 = velocity north-component\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_x=df_yrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_y=df_yrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gy=[df_yrot_x,df_yrot_y]\n",
    "    df_Gy=pd.concat(frames_Gy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "\n",
    "    df_zrot_x = df_zrot.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_zrot_y = df_zrot.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_zrot_x=df_zrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_zrot_y=df_zrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_zrot_x['flag']=np.array([1] * len(df_zrot_x)) # 1 = velocity east-component\n",
    "#     df_zrot_y['flag']=np.array([2] * len(df_zrot_y)) # 2 = velocity north-component\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_x=df_zrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_y=df_zrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gz=[df_zrot_x,df_zrot_y]\n",
    "    df_Gz=pd.concat(frames_Gz,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# SAVE G-matrix\n",
    "# Gmatrix = [Gxrot(1) Gyrot(1) Gzrot(1) ... Gxrot(HowMany) Gyrot(HowMany) Gzrot(HowMany)]\n",
    "    \n",
    "    df_G_BC_on_GNSS[\"G_xrot\"+str(i)] = df_Gx.loc[:,['velo']]\n",
    "    df_G_BC_on_GNSS[\"G_yrot\"+str(i)] = df_Gy.loc[:,['velo']]\n",
    "    df_G_BC_on_GNSS[\"G_zrot\"+str(i)] = df_Gz.loc[:,['velo']]\n",
    "    \n",
    "#df_G_BC_on_GNSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-b : Build G matrix part related to both Boundary Condition and Force Terms on GNSS data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_FT_on_GNSS_eij = pd.DataFrame(index = range(len(df_data_GNSS))) \n",
    "df_G_FT_on_GNSS_ezz = pd.DataFrame(index = range(len(df_data_GNSS))) \n",
    "# Make a blank G matrix part related to Boundary Condition on GNSS data points\n",
    "\n",
    "# print('How many grid cells do you have (where a set of 4 force terms defined) ? :')\n",
    "HowMany=360\n",
    "# print(\"360 (fixed for now [9/29/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_GNSS.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_GNSS.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_GNSS.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_hori_FT_\"+str(i)+\"_4\"+\"_on_GNSS.gmt\" # z  horizontal\n",
    "    \n",
    "## READ files into two separate structures in the following orders:\n",
    "## 1st stru = {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany} \n",
    "## 2nd stru = {z1 .. zHowMany}\n",
    "##\n",
    "## And then merge these two structures into one in the order of ...\n",
    "##            {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany, z1 .. zHowMany}\n",
    "\n",
    "\n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_exx.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gexx (i)\n",
    "\n",
    "    df_exx_x = df_exx.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_exx_y = df_exx.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_exx_x=df_exx_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exx_y=df_exx_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exx_x=df_exx_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_y=df_exx_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexx=[df_exx_x,df_exx_y]\n",
    "    df_Gexx=pd.concat(frames_Gexx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "# BUILD a column vector Geyy (i)\n",
    "\n",
    "    df_eyy_x = df_eyy.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_eyy_y = df_eyy.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_eyy_x=df_eyy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_eyy_y=df_eyy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_eyy_x=df_eyy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_y=df_eyy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Geyy=[df_eyy_x,df_eyy_y]\n",
    "    df_Geyy=pd.concat(frames_Geyy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gexy (i)\n",
    "\n",
    "    df_exy_x = df_exy.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_exy_y = df_exy.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_exy_x=df_exy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exy_y=df_exy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exy_x=df_exy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_y=df_exy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gexy=[df_exy_x,df_exy_y]\n",
    "    df_Gexy=pd.concat(frames_Gexy,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "# BUILD a column vector G_ezz (i)\n",
    "\n",
    "    df_ezz_x = df_ezz.iloc[:,[0,1,2]]  # saved vx basis function on the GNSS points\n",
    "    df_ezz_y = df_ezz.iloc[:,[0,1,3]]  # saved vn basis function on the GNSS points\n",
    "\n",
    "    df_ezz_x=df_ezz_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_ezz_y=df_ezz_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_ezz_x=df_ezz_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_y=df_ezz_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gezz=[df_ezz_x,df_ezz_y]\n",
    "    df_Gezz=pd.concat(frames_Gezz,ignore_index=True) # merge the two dataFrames into one (vertically)\n",
    "    \n",
    "    \n",
    "    \n",
    "# SAVE a part of G-matrix (as in two different structures and then they will be merged later)\n",
    "\n",
    "    # 1st structure = [Gexx(1) Geyy(1) Gexy(1) ... Gexx(HowMany) Geyy(HowMany) Gexy(HowMany)]   \n",
    "    df_G_FT_on_GNSS_eij[\"G_exx\"+str(i)] = df_Gexx.loc[:,['velo']]\n",
    "    df_G_FT_on_GNSS_eij[\"G_eyy\"+str(i)] = df_Geyy.loc[:,['velo']]\n",
    "    df_G_FT_on_GNSS_eij[\"G_exy\"+str(i)] = df_Gexy.loc[:,['velo']]\n",
    "\n",
    "    # 2nd structure = [Gezz(1) Gezz(2) ... Gezz(HowMany)] \n",
    "    df_G_FT_on_GNSS_ezz[\"G_ezz\"+str(i)] = df_Gezz.loc[:,['velo']]\n",
    "    \n",
    "    \n",
    "# Merge the two structures horizontally !\n",
    "\n",
    "frames_Geij_Gezz = [df_G_FT_on_GNSS_eij, df_G_FT_on_GNSS_ezz]\n",
    "df_G_FT_on_GNSS=pd.concat(frames_Geij_Gezz, axis=1) # merge the two dataFrames into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-c : Merge FT and BC basis functions horizontally. This G-matrix is for GNSS data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FT and BC basis functions horizontally !\n",
    "frames_FT_BC=[df_G_FT_on_GNSS, df_G_BC_on_GNSS]\n",
    "df_G_FT_BC_on_GNSS = pd.concat(frames_FT_BC, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-d : Build G matrix part only related to Boundary Condition on boundary points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_BC_on_boundary = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "# Make a blank G matrix part related to Boundary Condition on boundary points\n",
    "\n",
    "# print('How many files do you have for the boudnary basis functions? :')\n",
    "# HowMany = input()\n",
    "# HowMany = int(HowMany)\n",
    "#38 for the boundary condition (11/16/2021)\n",
    "HowMany=38\n",
    "# print(\"38 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_xrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_yrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gx (i)\n",
    "\n",
    "    df_xrot_x = df_xrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_xrot_y = df_xrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_xrot_x=df_xrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_xrot_y=df_xrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_xrot_x['flag']=np.array([1] * len(df_xrot_x)) # 1 = velocity east-component\n",
    "#     df_xrot_y['flag']=np.array([2] * len(df_xrot_y)) # 2 = velocity north-component\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_x=df_xrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_y=df_xrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx=[df_xrot_x,df_xrot_y]\n",
    "    df_Gx=pd.concat(frames_Gx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "\n",
    "    df_yrot_x = df_yrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_yrot_y = df_yrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_yrot_x=df_yrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_yrot_y=df_yrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_yrot_x['flag']=np.array([1] * len(df_yrot_x)) # 1 = velocity east-component\n",
    "#     df_yrot_y['flag']=np.array([2] * len(df_yrot_y)) # 2 = velocity north-component\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_x=df_yrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_y=df_yrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gy=[df_yrot_x,df_yrot_y]\n",
    "    df_Gy=pd.concat(frames_Gy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "\n",
    "    df_zrot_x = df_zrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_zrot_y = df_zrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_zrot_x=df_zrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_zrot_y=df_zrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_zrot_x['flag']=np.array([1] * len(df_zrot_x)) # 1 = velocity east-component\n",
    "#     df_zrot_y['flag']=np.array([2] * len(df_zrot_y)) # 2 = velocity north-component\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_x=df_zrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_y=df_zrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gz=[df_zrot_x,df_zrot_y]\n",
    "    df_Gz=pd.concat(frames_Gz,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# SAVE G-matrix\n",
    "# Gmatrix = [Gxrot(1) Gyrot(1) Gzrot(1) ... Gxrot(HowMany) Gyrot(HowMany) Gzrot(HowMany)]\n",
    "    \n",
    "    df_G_BC_on_boundary[\"G_xrot\"+str(i)] = df_Gx.loc[:,['velo']]\n",
    "    df_G_BC_on_boundary[\"G_yrot\"+str(i)] = df_Gy.loc[:,['velo']]\n",
    "    df_G_BC_on_boundary[\"G_zrot\"+str(i)] = df_Gz.loc[:,['velo']]\n",
    "    \n",
    "#df_G_BC_on_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-e : Build G matrix part related to both Boundary Condition and Force Terms on boundary points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_FT_on_boundary_eij = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "df_G_FT_on_boundary_ezz = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "# Make a blank G matrix part related to Boundary Condition on boundary points\n",
    "\n",
    "# print('How many grid cells do you have (where a set of 4 force terms defined) ? :')\n",
    "HowMany=360\n",
    "# print(\"360 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_boundary.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_boundary.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_boundary.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_hori_FT_\"+str(i)+\"_4\"+\"_on_boundary.gmt\" # z  horizontal\n",
    "    \n",
    "## READ files into two separate structures in the following orders:\n",
    "## 1st stru = {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany} \n",
    "## 2nd stru = {z1 .. zHowMany}\n",
    "##\n",
    "## And then merge these two structures into one in the order of ...\n",
    "##            {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany, z1 .. zHowMany}\n",
    "\n",
    "\n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_exx.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gexx (i)\n",
    "\n",
    "    df_exx_x = df_exx.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_exx_y = df_exx.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_exx_x=df_exx_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exx_y=df_exx_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exx_x=df_exx_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_y=df_exx_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexx=[df_exx_x,df_exx_y]\n",
    "    df_Gexx=pd.concat(frames_Gexx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "# BUILD a column vector Geyy (i)\n",
    "\n",
    "    df_eyy_x = df_eyy.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_eyy_y = df_eyy.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_eyy_x=df_eyy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_eyy_y=df_eyy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_eyy_x=df_eyy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_y=df_eyy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Geyy=[df_eyy_x,df_eyy_y]\n",
    "    df_Geyy=pd.concat(frames_Geyy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gexy (i)\n",
    "\n",
    "    df_exy_x = df_exy.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_exy_y = df_exy.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_exy_x=df_exy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exy_y=df_exy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exy_x=df_exy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_y=df_exy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gexy=[df_exy_x,df_exy_y]\n",
    "    df_Gexy=pd.concat(frames_Gexy,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "# BUILD a column vector G_ezz (i)\n",
    "\n",
    "    df_ezz_x = df_ezz.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_ezz_y = df_ezz.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_ezz_x=df_ezz_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_ezz_y=df_ezz_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_ezz_x=df_ezz_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_y=df_ezz_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gezz=[df_ezz_x,df_ezz_y]\n",
    "    df_Gezz=pd.concat(frames_Gezz,ignore_index=True) # merge the two dataFrames into one (vertically)\n",
    "    \n",
    "    \n",
    "    \n",
    "# SAVE a part of G-matrix (as in two different structures and then they will be merged later)\n",
    "\n",
    "    # 1st structure = [Gexx(1) Geyy(1) Gexy(1) ... Gexx(HowMany) Geyy(HowMany) Gexy(HowMany)]   \n",
    "    df_G_FT_on_boundary_eij[\"G_exx\"+str(i)] = df_Gexx.loc[:,['velo']]\n",
    "    df_G_FT_on_boundary_eij[\"G_eyy\"+str(i)] = df_Geyy.loc[:,['velo']]\n",
    "    df_G_FT_on_boundary_eij[\"G_exy\"+str(i)] = df_Gexy.loc[:,['velo']]\n",
    "\n",
    "    # 2nd structure = [Gezz(1) Gezz(2) ... Gezz(HowMany)] \n",
    "    df_G_FT_on_boundary_ezz[\"G_ezz\"+str(i)] = df_Gezz.loc[:,['velo']]\n",
    "    \n",
    "    \n",
    "# Merge the two structures horizontally !\n",
    "\n",
    "frames_Geij_Gezz = [df_G_FT_on_boundary_eij, df_G_FT_on_boundary_ezz]\n",
    "df_G_FT_on_boundary=pd.concat(frames_Geij_Gezz, axis=1) # merge the two dataFrames into one\n",
    "#df_G_FT_on_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-f : Merge FT and BC basis functions horizontally. This G-matrix is for boundary velocity points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FT and BC basis functions horizontally !\n",
    "frames_FT_BC=[df_G_FT_on_boundary, df_G_BC_on_boundary]\n",
    "df_G_FT_BC_on_boundary = pd.concat(frames_FT_BC, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)\n",
    "#df_G_FT_BC_on_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-g : Build G matrix part only related to Force Terms on InSAR data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_FT_on_InSAR_hori = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "df_G_FT_on_InSAR_vert = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "# Make a blank G matrix part related to Force Terms on InSAR data points\n",
    "\n",
    "# df_G_FT_on_InSAR :\n",
    "# will be 'concat'ed with a frame of [df_G_FT_on_InSAR_hori, df_G_FT_on_InSAR_vert] and with axis=1\n",
    "\n",
    "# df_data_InSAR : velocity only\n",
    "# df_data_InSAR_all : lon lat vel px py pz\n",
    "\n",
    "df_px = df_data_InSAR_all.iloc[:,[3]]\n",
    "df_py = df_data_InSAR_all.iloc[:,[4]]\n",
    "df_pz = df_data_InSAR_all.iloc[:,[5]]\n",
    "\n",
    "#print('How many grid cells do you have (where a set of 4 force terms defined) ? :'\n",
    "HowMany=360\n",
    "#print(\"360 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_InSAR.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_InSAR.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_InSAR.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_vert_FT_\"+str(i)+\"_4\"+\"_on_InSAR.gmt\" # z  vertical\n",
    "    \n",
    "## READ files into two separate structures in the following orders:\n",
    "## 1st stru = {exx1_x*px + exx1_y*py, eyy1_x*px + eyy1_y*py , exy1_x*px + exy1_y*px, ..., \n",
    "##             eyyHowMany_x*px + eyyHowMany_y*py, exyHowMany_x*px + exyHowMany_y*py} \n",
    "## 2nd stru = {ezz1_z*pz, ezz2_z*pz, ..., ezzHowMany_z*pz}\n",
    "##\n",
    "## And then merge these two structures into one in the order of ...\n",
    "##            {exx1_x*px + exx1_y*py,...,exyHowMany_x*px + exyHowMany_y*py,ezz1_z*pz, ..., ezzHowMany_z*pz}\n",
    "\n",
    "\n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    # ve = Px\n",
    "    # vn = Py\n",
    "    # vz = Pz \n",
    "    # This is because later on ...\n",
    "    # 've' will be multiplied by a dataFrame named 'Px'\n",
    "    # 'vn' will be multiplied by a dataFrame named 'Py'\n",
    "    # 'vz' will be multiplied by a dataFrame named 'Pz'\n",
    "    # pandas does Not allow to multiply (element wise) two different df in different names\n",
    "    \n",
    "    df_exx.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','Pz']\n",
    "\n",
    "# SORT THEM by the same order of the InSAR data,\n",
    "# which was already sorted in the beginning of this code.\n",
    "    \n",
    "    \n",
    "    df_exx=df_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy=df_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy=df_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz=df_ezz.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# STACK the data frames of the basis function responses vertically!\n",
    "# InSAR data vector made of 2 separate pointing vectors\n",
    "# but the 2 data sets are defined in the same coordinates. \n",
    "# One needs to stack 2 of df_eij vertically. \n",
    "# len(df_eij) = len(InSAR_data) / 2\n",
    "\n",
    "# len(df_eij_stacked) = len(InSAR_data) !!\n",
    "     \n",
    "    \n",
    "    frames_exx_stack = [df_exx,df_exx] # Two insar data sets!\n",
    "    df_exx_stacked=pd.concat(frames_exx_stack,ignore_index=True) \n",
    "    # merge the two dataFrames into one\n",
    "\n",
    "    frames_eyy_stack = [df_eyy,df_eyy] # Two insar data sets!\n",
    "    df_eyy_stacked=pd.concat(frames_eyy_stack,ignore_index=True) \n",
    "    # merge the two dataFrames into one    \n",
    "\n",
    "    frames_exy_stack = [df_exy,df_exy] # Two insar data sets!\n",
    "    df_exy_stacked=pd.concat(frames_exy_stack,ignore_index=True) \n",
    "    # merge the two dataFrames into one\n",
    "\n",
    "    frames_ezz_stack = [df_ezz,df_ezz] # Two insar data sets!\n",
    "    df_ezz_stacked=pd.concat(frames_ezz_stack,ignore_index=True) \n",
    "    # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# # BUILD a column vector Gx (i)\n",
    "    df_LOS_Gexx_x = df_exx_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gexx_y = df_exx_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gexx_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gexx = df_LOS_Gexx_x + df_LOS_Gexx_y\n",
    "    \n",
    "    \n",
    "    df_LOS_Geyy_x = df_eyy_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Geyy_y = df_eyy_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Geyy_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Geyy = df_LOS_Geyy_x + df_LOS_Geyy_y\n",
    "    \n",
    "    \n",
    "    df_LOS_Gexy_x = df_exy_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gexy_y = df_exy_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gexy_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gexy = df_LOS_Gexy_x + df_LOS_Gexy_y\n",
    "\n",
    "    \n",
    "    df_LOS_Gezz = df_ezz_stacked.loc[:,['Pz']] * df_pz\n",
    "    \n",
    "\n",
    "# SAVE G-matrix\n",
    "# df_G_FT_on_InSAR_hori = [(exx1_x*px + exx1_y*py), ... , (exyHowMany_x*px + exyHowMany_y*py)]\n",
    "\n",
    "    \n",
    "    df_G_FT_on_InSAR_hori[\"GexxXPx+GexxYPy \"+str(i)] = df_LOS_Gexx.loc[:,['Py']]\n",
    "    df_G_FT_on_InSAR_hori[\"GeyyXPx+GeyyYPy \"+str(i)] = df_LOS_Geyy.loc[:,['Py']]\n",
    "    df_G_FT_on_InSAR_hori[\"GexyXPx+GexyYPy \"+str(i)] = df_LOS_Gexy.loc[:,['Py']]\n",
    "    \n",
    "    df_G_FT_on_InSAR_vert[\"GezzZPz \"+str(i)] = df_LOS_Gezz['Pz']\n",
    "    \n",
    "    \n",
    "frames_FT_InSAR=[df_G_FT_on_InSAR_hori, df_G_FT_on_InSAR_vert]\n",
    "df_G_FT_on_InSAR = pd.concat(frames_FT_InSAR, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-h : Build G matrix part related to both Force Terms and Boundary Conditions on InSAR data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_BC_on_InSAR = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "# Make a blank G matrix part related to Bounndary Condition on InSAR data points\n",
    "\n",
    "# df_G_BC_on_InSAR :\n",
    "\n",
    "# df_data_InSAR : velocity only\n",
    "# df_data_InSAR_all : lon lat vel px py pz\n",
    "\n",
    "df_px = df_data_InSAR_all.iloc[:,[3]]\n",
    "df_py = df_data_InSAR_all.iloc[:,[4]]\n",
    "df_pz = df_data_InSAR_all.iloc[:,[5]]\n",
    "\n",
    "#print('How many files do you have for the boudnary basis functions? :')\n",
    "#38 for the boundary condition (11/16/2021)\n",
    "HowMany=38\n",
    "#print(\"38 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "    # ve = Px\n",
    "    # vn = Py\n",
    "    # vz = Pz \n",
    "    # This is because later on ...\n",
    "    # 've' will be multiplied by a dataFrame named 'Px'\n",
    "    # 'vn' will be multiplied by a dataFrame named 'Py'\n",
    "    # 'vz' will be multiplied by a dataFrame named 'Pz'\n",
    "    # pandas does Not allow to multiply (element wise) two different df in different names\n",
    "\n",
    "    df_xrot.columns = ['lon','lat','Px','Py','se','sn','corr']  \n",
    "    df_yrot.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    \n",
    "\n",
    "    \n",
    "# SORT THEM by the same order of the InSAR data,\n",
    "# which was already sorted in the beginning of this code.\n",
    "\n",
    "    df_xrot=df_xrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot=df_yrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot=df_zrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# STACK the data frames of the basis function responses vertically!\n",
    "# InSAR data vector made of 2 separate pointing vectors\n",
    "# but the 2 data sets are defined in the same coordinates. \n",
    "# One needs to stack 2 of df_irot vertically. \n",
    "# len(df_xrot) = len(InSAR_data)/2 \n",
    "    \n",
    "# len(df_xrot_stacked) = len(InSAR_data) !!\n",
    "    \n",
    "    frames_xrot_stack = [df_xrot,df_xrot]\n",
    "    df_xrot_stacked=pd.concat(frames_xrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    frames_yrot_stack = [df_yrot,df_yrot]\n",
    "    df_yrot_stacked=pd.concat(frames_yrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "    frames_zrot_stack = [df_zrot,df_zrot]\n",
    "    df_zrot_stacked=pd.concat(frames_zrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "    \n",
    "# # BUILD a column vector Gx (i)\n",
    "    df_LOS_Gxrot_x = df_xrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gxrot_y = df_xrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gxrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gxrot = df_LOS_Gxrot_x + df_LOS_Gxrot_y\n",
    "    \n",
    "\n",
    "    df_LOS_Gyrot_x = df_yrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gyrot_y = df_yrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gyrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gyrot = df_LOS_Gyrot_x + df_LOS_Gyrot_y\n",
    "    \n",
    "\n",
    "    df_LOS_Gzrot_x = df_zrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gzrot_y = df_zrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gzrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gzrot = df_LOS_Gzrot_x + df_LOS_Gzrot_y\n",
    "    \n",
    "\n",
    "# SAVE G-matrix\n",
    "# df_G_FT_on_InSAR_hori = [(xrot1_x*px + xrot1_y*py), ... , (zrotHowMany_x*px + zrotHowMany_y*py)]\n",
    "\n",
    "    \n",
    "    df_G_BC_on_InSAR[\"GxrotXPx+GxrotYPy \"+str(i)] = df_LOS_Gxrot.loc[:,['Py']]\n",
    "    df_G_BC_on_InSAR[\"GyrotXPx+GyrotYPy \"+str(i)] = df_LOS_Gyrot.loc[:,['Py']]\n",
    "    df_G_BC_on_InSAR[\"GzrotXPx+GzrotYPy \"+str(i)] = df_LOS_Gzrot.loc[:,['Py']]\n",
    "    \n",
    "\n",
    "#df_G_BC_on_InSAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-i : Merge FT and BC basis functions horizontally. This G-matrix is for InSAR points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FT and BC basis functions on InSAR data point horizontally !\n",
    "frames_FT_BC_on_InSAR=[df_G_FT_on_InSAR, df_G_BC_on_InSAR]\n",
    "df_G_FT_BC_on_InSAR = pd.concat(frames_FT_BC_on_InSAR, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)\n",
    "#df_G_FT_BC_on_InSAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 2-FINAL : Build the complete G matrix `\n",
    "    \n",
    "### G-matrix = [df_G_FT_BC_on_GNSS; df_G_FT_BC_on_InSAR; df_G_FT_BC_on_boundary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_FT_BC_on_boundary.columns = df_G_FT_BC_on_InSAR.columns\n",
    "df_G_FT_BC_on_GNSS.columns = df_G_FT_BC_on_InSAR.columns\n",
    "# change column names of the df_G_FT_BC_on_boundary\n",
    "# change column names of the df_G_FT_BC_on_GNSS\n",
    "\n",
    "frames_FT_BC_final=[df_G_FT_BC_on_GNSS, df_G_FT_BC_on_InSAR, df_G_FT_BC_on_boundary]\n",
    "df_G_final = pd.concat(frames_FT_BC_final, ignore_index=True) #merge the two dataFrames into one vertically (axis = 1)\n",
    "#df_G_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_data_total)!=len(df_G_final):\n",
    "    print(\"WARNING: Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `**STEP3**` : Joint Inversion of GNSS, InSAR data and Boundary velocity field\n",
    "> G-matrix = **df_G_final** \\\n",
    "> data vec = **df_data_total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Diagonal Weighting Matrix W\n",
    "nGNSS=len(df_data_GNSS)\n",
    "nBC=len(df_data_BC)\n",
    "nInSAR=len(df_data_InSAR)\n",
    "nTotal=len(df_data_total)\n",
    "\n",
    "errorGNSS = np.ones(nGNSS)*weight_for_GNSS\n",
    "errorInSAR = np.ones(nInSAR)*weight_for_InSAR \n",
    "errorBC = np.ones(nBC)*weight_for_BC\n",
    "\n",
    "errorTotal = np.concatenate((errorGNSS,errorInSAR, errorBC),axis=0)\n",
    "errorTotalinv = 1/errorTotal\n",
    "W = np.diag(errorTotalinv)\n",
    "\n",
    "# convert into a dataframe\n",
    "dfW = pd.DataFrame(W)\n",
    "\n",
    "# When calculating predictions, the non-weighted G-matrix is needed. \n",
    "# SAVE it!\n",
    "df_G_final_save = df_G_final\n",
    "\n",
    "# When calculating the misfit, the non-weighted data is needed. \n",
    "# SAVE it!\n",
    "df_data_total_save = df_data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the Diagonal Weighting Matrix dfW to the data vector and Gmatrix\n",
    "\n",
    "df_G_final = dfW @ df_G_final_save\n",
    "df_data_total = dfW @ df_data_total_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_prime = df_G_final.transpose() \n",
    "\n",
    "# G'G\n",
    "# >Two different ways to compute a matrix multiplication\n",
    "# >1st method\n",
    "GpG1=df_G_prime.dot(df_G_final) #G'G\n",
    "# >2nd method\n",
    "GpG2=df_G_prime @ df_G_final #G'G\n",
    "# >These results are same.\n",
    "# >Let's take the second one as G'G\n",
    "GpG = GpG2 #GpG is G'G\n",
    "# inv(G'G)\n",
    "# > Two different ways to obtain inverse matrix\n",
    "# > 1st method: np.linalg.inv \n",
    "\n",
    "\n",
    "###############################\n",
    "## inv(G'G)*G'*d = model(LSM) #\n",
    "###############################\n",
    "if inversion_flag == 1:\n",
    "    # > 1st method: np.linalg.inv\n",
    "    df_inv_GpG = pd.DataFrame(np.linalg.inv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_inv_GpG@df_G_prime@df_data_total #inversion\n",
    "    \n",
    "elif inversion_flag == 2:\n",
    "    # > 2nd method: np.linalg.pinv (Moore-Penrose inverse (SVD))\n",
    "    df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion\n",
    "else:\n",
    "    # > 3rd method: Damping  \n",
    "    alp=damping_for_horizontal*np.ones((360*3,1))**2 # damping parameter for hori\n",
    "    bet=damping_for_vertical*np.ones((360,1))**2 # damping parameter for vert\n",
    "    gam=damping_for_rotation*np.ones((38*3,1))**2 # damping parameter for rot\n",
    "    array_tuple = (alp, bet, gam)\n",
    "    lamb = np.vstack(array_tuple) \n",
    "    lamb = lamb[:,0]\n",
    "    damping_matrix=np.diag(lamb) # a*a*I\n",
    "    \n",
    "    \n",
    "    GpG_damping = GpG + damping_matrix #(G'G + a*a*I)\n",
    "    GpD=df_G_prime@df_data_total\n",
    "    df_model_damping= np.linalg.solve(GpG_damping,GpD)\n",
    "    df_model1=pd.DataFrame(df_model_damping[:,0])\n",
    "    df_model1.index=df_G_final_save.columns.values\n",
    "## data predicted.\n",
    "#df_data_predicted = df_G_final@df_model1 \n",
    "df_data_predicted = df_G_final_save @ df_model1\n",
    "# df_G_final_save is the non-weighted G-matrix\n",
    "\n",
    "\n",
    "## norm2 misfit\n",
    "df_norm2=(df_data_total_save.to_numpy()-df_data_predicted.to_numpy())**2\n",
    "df_norm2=df_norm2.sum()\n",
    "#df_norm2=np.lib.scimath.sqrt(df_norm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predicted data vector into\n",
    "# (1) GNSS x and y\n",
    "# (2) InSAR 1\n",
    "# (3) InSAR 2\n",
    "# (4) BC vel x and y\n",
    "\n",
    "num_velo_point0=len(df_data_GNSS_all) # GNSS\n",
    "num_velo_point0=int(num_velo_point0)\n",
    "\n",
    "num_velo_point1=len(df_data_InSAR_all)/2  # 2 InSAR data sets in a column vector\n",
    "num_velo_point1=int(num_velo_point1)\n",
    "\n",
    "num_velo_point2=len(df_data_BC_all) #BC vel\n",
    "num_velo_point2=int(num_velo_point2)\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "#####  GNSS  #######\n",
    "####################\n",
    "\n",
    "#GNSS predicted x\n",
    "df_prediction_GNSS_x=df_data_predicted.iloc[0:int(num_velo_point0/2)] \n",
    "df_prediction_GNSS_x=df_prediction_GNSS_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_GNSS_y=df_data_predicted.iloc[int(num_velo_point0/2):num_velo_point0] \n",
    "df_prediction_GNSS_y=df_prediction_GNSS_y.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_GNSS_save = df_data_GNSS_all.iloc[0:int(num_velo_point0/2),[0,1]]\n",
    "\n",
    "df_save_GNSS_XandY = df_GNSS_save.reset_index(drop=True)\n",
    "df_save_GNSS_XandY['vx'] = df_prediction_GNSS_x\n",
    "df_save_GNSS_XandY['vn'] = df_prediction_GNSS_y\n",
    "df_save_GNSS_XandY['se'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['sn'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['corr'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#####  INSAR  #######\n",
    "#####################\n",
    "\n",
    "df_prediction_D=df_data_predicted.iloc[num_velo_point0:num_velo_point0+num_velo_point1] #dLOS Descending\n",
    "df_prediction_D=df_prediction_D.reset_index(drop=True)\n",
    "\n",
    "df_prediction_A=df_data_predicted.iloc[num_velo_point0+num_velo_point1:num_velo_point0+2*num_velo_point1] #dLOS Ascending\n",
    "df_prediction_A=df_prediction_A.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted InSAR Data\n",
    "df_InSAR_save = df_inputInSAR1.iloc[:,[0,1]]   \n",
    "\n",
    "df_save_D = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_D['dLOS'] = df_prediction_D #append predicted dLOS Descending\n",
    "df_save_D['dLOS'] = df_save_D['dLOS']\n",
    "\n",
    "df_save_A = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A['dLOS'] = df_prediction_A #append predicted dLOS Ascending\n",
    "df_save_A['dLOS'] = df_save_A['dLOS']\n",
    "\n",
    "########################\n",
    "#####  Boundary ########\n",
    "########################\n",
    "\n",
    "#BC predicted x\n",
    "df_prediction_BC_x=df_data_predicted.iloc[num_velo_point0+2*num_velo_point1:num_velo_point0+2*num_velo_point1+int(num_velo_point2/2)] \n",
    "df_prediction_BC_x=df_prediction_BC_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_BC_y=df_data_predicted.iloc[num_velo_point0+2*num_velo_point1+int(num_velo_point2/2):num_velo_point0+2*num_velo_point1+num_velo_point2] \n",
    "df_prediction_BC_y=df_prediction_BC_y.reset_index(drop=True)\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_BC_save = df_data_BC_all.iloc[0:int(num_velo_point2/2),[0,1]]\n",
    "df_save_BC_XandY = df_BC_save.reset_index(drop=True)\n",
    "df_save_BC_XandY['vx'] = df_prediction_BC_x\n",
    "df_save_BC_XandY['vn'] = df_prediction_BC_y\n",
    "df_save_BC_XandY['se'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['sn'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['corr'] = np.zeros(len(df_prediction_BC_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#########################################################################################\n",
    "#################################   SAVE FILES       ####################################\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "#  SAVE predicted GNSS velocity values on the GNSS data points in ...\n",
    "#  **`vel_GNSS_rotated_pred.gmt`**\n",
    "\n",
    "#  SAVE predicted InSAR velocity values on the InSAR data points in ...                     \n",
    "#  **`DT42_data_new_ref_pred.dat`** &\n",
    "#  **`AT35_data_new_ref_pred.dat`**\n",
    "\n",
    "#  SAVE predicted BC velocity values on the boundary data points in ...\n",
    "#  **`vel_BC_pred.dat`**\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "#df_data_GNSS_all\n",
    "#df_data_InSAR_all\n",
    "#df_data_BC_all\n",
    "\n",
    "\n",
    "outputFILE_GNSS_XandY=\"vel_GNSS_rotated_pred.gmt\"\n",
    "df_save_GNSS_XandY.to_csv(outputFILE_GNSS_XandY, header=None, index=None, sep=' ', float_format='%g')\n",
    "\n",
    "outputFILE_D=\"DT42_data_new_ref_pred.dat\"\n",
    "df_save_D.to_csv(outputFILE_D, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_A=\"AT35_data_new_ref_pred.dat\"\n",
    "df_save_A.to_csv(outputFILE_A, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_BC_XandY=\"vel_BC_pred.dat\"\n",
    "df_save_BC_XandY.to_csv(outputFILE_BC_XandY, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<div class=\"alert--icon\"> <i class=\"far fa-times-circle\"></i> </div>\n",
    "    <p> Simple LSM is not able to recover BC velocity well. Try weighted inversion </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <p> The weighted LSM works well. </p>\n",
    "    <b> There is tradeoff between data sets near the edges though. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-square statistics is : 47.764907 [mm/yr]**2 when the weighting factor for InSAR and GNSS were 1 and 1, respectively\n"
     ]
    }
   ],
   "source": [
    "print(\"chi-square statistics is : %f [mm/yr]**2 when the weighting factor for InSAR and GNSS were %i and %i, respectively\"  % (df_norm2, weight_for_InSAR,weight_for_GNSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFILE_model=\"model_coef.dat\"\n",
    "df_model1.to_csv(outputFILE_model, header=None, index=None, float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`STEP 4`** : Get the corresponding 3-D model field using the model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "#                          Force Balance Equation                                      #\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "knotpoints_sample = np.loadtxt('vel_hori_FT_1_1_on_knotpoints.gmt')\n",
    "knotpoints_XandY=len(knotpoints_sample)*2\n",
    "knotpoints_Z=len(knotpoints_sample)\n",
    "\n",
    "df_G_FT_on_knotpoints_eij = pd.DataFrame(index = range(knotpoints_XandY)) \n",
    "df_G_FT_on_knotpoints_ezz = pd.DataFrame(index = range(knotpoints_XandY)) \n",
    "df_G_FT_on_knotpoints_zzz = pd.DataFrame(index = range(knotpoints_Z))\n",
    "# Make a blank G matrix part for velocity\n",
    "\n",
    "midpoints_sample = np.loadtxt('average_strain_FT_1_1_RECTANGULAR.out')\n",
    "midpoints_XXandYYandXY = len(midpoints_sample)*3\n",
    "\n",
    "df_G_FT_on_midpoints_eij =pd.DataFrame(index = range(midpoints_XXandYYandXY))\n",
    "df_G_FT_on_midpoints_ezz =pd.DataFrame(index = range(midpoints_XXandYYandXY))\n",
    "# Make a blank G matrix part for strain\n",
    "\n",
    "# print('How many grid cells do you have (where a set of 4 force terms defined) ? :')\n",
    "HowMany=360\n",
    "# print(\"360 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_knotpoints.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_knotpoints.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_knotpoints.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_hori_FT_\"+str(i)+\"_4\"+\"_on_knotpoints.gmt\" # z  horizontal   \n",
    "    inputfile_zzz = \"vel_vert_FT_\"+str(i)+\"_4\"+\"_on_knotpoints.gmt\" # z  vertical\n",
    "    \n",
    "    \n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "    df_zzz=pd.read_csv(inputfile_zzz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "    \n",
    "# CHANGE the column names \n",
    "\n",
    "    df_exx.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zzz.columns = ['lon','lat','vz']\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gexx (i)\n",
    "\n",
    "    df_exx_x = df_exx.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_exx_y = df_exx.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_exx_x=df_exx_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exx_y=df_exx_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exx_x=df_exx_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_y=df_exx_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexx=[df_exx_x,df_exx_y]\n",
    "    df_Gexx=pd.concat(frames_Gexx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "# BUILD a column vector Geyy (i)\n",
    "\n",
    "    df_eyy_x = df_eyy.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_eyy_y = df_eyy.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_eyy_x=df_eyy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_eyy_y=df_eyy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_eyy_x=df_eyy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_y=df_eyy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Geyy=[df_eyy_x,df_eyy_y]\n",
    "    df_Geyy=pd.concat(frames_Geyy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gexy (i)\n",
    "\n",
    "    df_exy_x = df_exy.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_exy_y = df_exy.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_exy_x=df_exy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exy_y=df_exy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exy_x=df_exy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_y=df_exy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gexy=[df_exy_x,df_exy_y]\n",
    "    df_Gexy=pd.concat(frames_Gexy,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "# BUILD a column vector G_ezz (i)\n",
    "\n",
    "    df_ezz_x = df_ezz.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_ezz_y = df_ezz.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_ezz_x=df_ezz_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_ezz_y=df_ezz_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_ezz_x=df_ezz_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_y=df_ezz_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gezz=[df_ezz_x,df_ezz_y]\n",
    "    df_Gezz=pd.concat(frames_Gezz,ignore_index=True) # merge the two dataFrames into one (vertically)\n",
    "    \n",
    "# BUILD a column vector G_zzz (i)   \n",
    "    df_zzz=df_zzz.sort_values(['lat','lon'], ascending=[True, True])\n",
    "    df_zzz=df_zzz.reset_index(drop=True)\n",
    "# SAVE a part of G-matrix (as in two different structures and then they will be merged later)\n",
    "\n",
    "    # 1st structure = [Gexx(1) Geyy(1) Gexy(1) ... Gexx(HowMany) Geyy(HowMany) Gexy(HowMany)]   \n",
    "    df_G_FT_on_knotpoints_eij[\"G_exx\"+str(i)] = df_Gexx.loc[:,['velo']]\n",
    "    df_G_FT_on_knotpoints_eij[\"G_eyy\"+str(i)] = df_Geyy.loc[:,['velo']]\n",
    "    df_G_FT_on_knotpoints_eij[\"G_exy\"+str(i)] = df_Gexy.loc[:,['velo']]\n",
    "\n",
    "    # 2nd structure = [Gezz(1) Gezz(2) ... Gezz(HowMany)] \n",
    "    df_G_FT_on_knotpoints_ezz[\"G_ezz\"+str(i)] = df_Gezz.loc[:,['velo']]\n",
    "    df_G_FT_on_knotpoints_zzz[\"G_zzz\"+str(i)] = df_zzz.loc[:,['vz']]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#########STRAIN ###########\n",
    "    inputfile_strain_exx = \"average_strain_FT_\"+str(i)+\"_1_RECTANGULAR.out\" #exx strain \n",
    "    inputfile_strain_eyy = \"average_strain_FT_\"+str(i)+\"_2_RECTANGULAR.out\" #eyy strain \n",
    "    inputfile_strain_exy = \"average_strain_FT_\"+str(i)+\"_3_RECTANGULAR.out\" #exy strain \n",
    "    inputfile_strain_ezz = \"average_strain_FT_\"+str(i)+\"_4_RECTANGULAR.out\" #ezz strain \n",
    "\n",
    "    \n",
    "    df_exx_strain=pd.read_csv(inputfile_strain_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy_strain=pd.read_csv(inputfile_strain_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy_strain=pd.read_csv(inputfile_strain_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz_strain=pd.read_csv(inputfile_strain_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "    \n",
    "    df_exx_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    df_eyy_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    df_exy_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    df_ezz_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    \n",
    "    # BUILD a column vector Gexx (i) : STRAIN\n",
    "    df_exx_exx = df_exx_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints\n",
    "    df_exx_eyy = df_exx_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints\n",
    "    df_exx_exy = df_exx_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints\n",
    "    df_exx_exx=df_exx_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_exx_eyy=df_exx_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_exx_exy=df_exx_exy.rename(columns ={'exy': 'strain'}) #column name change  \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exx_exx=df_exx_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_eyy=df_exx_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_exy=df_exx_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexx_strain=[df_exx_exx,df_exx_eyy,df_exx_exy]\n",
    "    df_Gexx_strain=pd.concat(frames_Gexx_strain,ignore_index=True) # merge the three dataFrames into one\n",
    "    \n",
    "    # BUILD a column vector Geyy (i) : STRAIN\n",
    "    df_eyy_exx = df_eyy_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints\n",
    "    df_eyy_eyy = df_eyy_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints\n",
    "    df_eyy_exy = df_eyy_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints\n",
    "    df_eyy_exx=df_eyy_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_eyy_eyy=df_eyy_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_eyy_exy=df_eyy_exy.rename(columns ={'exy': 'strain'}) #column name change  \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_eyy_exx=df_eyy_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_eyy=df_eyy_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_exy=df_eyy_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Geyy_strain=[df_eyy_exx,df_eyy_eyy,df_eyy_exy]\n",
    "    df_Geyy_strain=pd.concat(frames_Geyy_strain,ignore_index=True) # merge the three dataFrames into one    \n",
    "    \n",
    "\n",
    "    # BUILD a column vector Gexy (i) : STRAIN\n",
    "    df_exy_exx = df_exy_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints\n",
    "    df_exy_eyy = df_exy_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints\n",
    "    df_exy_exy = df_exy_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints\n",
    "    df_exy_exx=df_exy_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_exy_eyy=df_exy_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_exy_exy=df_exy_exy.rename(columns ={'exy': 'strain'}) #column name change  \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exy_exx=df_exy_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_eyy=df_exy_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_exy=df_exy_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexy_strain=[df_exy_exx,df_exy_eyy,df_exy_exy]\n",
    "    df_Gexy_strain=pd.concat(frames_Gexy_strain,ignore_index=True) # merge the three dataFrames into one   \n",
    "    \n",
    "\n",
    "    # BUILD a column vector Gezz (i) : STRAIN\n",
    "    df_ezz_exx = df_ezz_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints\n",
    "    df_ezz_eyy = df_ezz_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints\n",
    "    df_ezz_exy = df_ezz_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints\n",
    "    df_ezz_exx=df_ezz_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_ezz_eyy=df_ezz_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_ezz_exy=df_ezz_exy.rename(columns ={'exy': 'strain'}) #column name change  \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_ezz_exx=df_ezz_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_eyy=df_ezz_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_exy=df_ezz_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gezz_strain=[df_ezz_exx,df_ezz_eyy,df_ezz_exy]\n",
    "    df_Gezz_strain=pd.concat(frames_Gezz_strain,ignore_index=True) # merge the three dataFrames into one     \n",
    "    \n",
    "    \n",
    "\n",
    "    df_G_FT_on_midpoints_eij[\"G_exx\"+str(i)] = df_Gexx_strain.loc[:,['strain']]\n",
    "    df_G_FT_on_midpoints_eij[\"G_eyy\"+str(i)] = df_Geyy_strain.loc[:,['strain']]\n",
    "    df_G_FT_on_midpoints_eij[\"G_exy\"+str(i)] = df_Gexy_strain.loc[:,['strain']]\n",
    "\n",
    "    # 2nd structure = [Gezz(1) Gezz(2) ... Gezz(HowMany)] \n",
    "    df_G_FT_on_midpoints_ezz[\"G_ezz\"+str(i)] = df_Gezz_strain.loc[:,['strain']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Merge the two structures horizontally !\n",
    "frames_Geij_Gezz = [df_G_FT_on_knotpoints_eij, df_G_FT_on_knotpoints_ezz]\n",
    "df_G_FT_on_knotpoints=pd.concat(frames_Geij_Gezz, axis=1) # merge the two dataFrames into one\n",
    "#df_G_FT_on_knotpoints #horizontal\n",
    "#df_G_FT_on_knotpoints_zzz #vertical\n",
    "\n",
    "# Merge the two structures horizontally ! FOR strain\n",
    "frames_Geij_Gezz_strain = [df_G_FT_on_midpoints_eij, df_G_FT_on_midpoints_ezz]\n",
    "df_G_FT_on_midpoints_strain=pd.concat(frames_Geij_Gezz_strain, axis=1) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "#                          Boundary Rotation velo       & strain                       #\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "df_G_BC_on_knotpoints = pd.DataFrame(index = range(knotpoints_XandY)) \n",
    "# Make a blank G matrix part related to Boundary Condition on the knotpoints\n",
    "\n",
    "\n",
    "df_G_BC_on_midpoints_strain =pd.DataFrame(index = range(midpoints_XXandYYandXY))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('How many files do you have for the boudnary basis functions? :')\n",
    "# HowMany = input()\n",
    "# HowMany = int(HowMany)\n",
    "#38 for the boundary condition (11/16/2021)\n",
    "HowMany=38\n",
    "# print(\"38 (fixed for now [11/16/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_knotpoints.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_knotpoints.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_knotpoints.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_xrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_yrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gx (i)\n",
    "\n",
    "    df_xrot_x = df_xrot.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_xrot_y = df_xrot.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_xrot_x=df_xrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_xrot_y=df_xrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_x=df_xrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_y=df_xrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx=[df_xrot_x,df_xrot_y]\n",
    "    df_Gx=pd.concat(frames_Gx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "\n",
    "    df_yrot_x = df_yrot.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_yrot_y = df_yrot.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_yrot_x=df_yrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_yrot_y=df_yrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_x=df_yrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_y=df_yrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gy=[df_yrot_x,df_yrot_y]\n",
    "    df_Gy=pd.concat(frames_Gy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "\n",
    "    df_zrot_x = df_zrot.iloc[:,[0,1,2]]  # saved vx basis function on the knotpoints\n",
    "    df_zrot_y = df_zrot.iloc[:,[0,1,3]]  # saved vn basis function on the knotpoints\n",
    "\n",
    "    df_zrot_x=df_zrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_zrot_y=df_zrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_x=df_zrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_y=df_zrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gz=[df_zrot_x,df_zrot_y]\n",
    "    df_Gz=pd.concat(frames_Gz,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# SAVE G-matrix\n",
    "# Gmatrix = [Gxrot(1) Gyrot(1) Gzrot(1) ... Gxrot(HowMany) Gyrot(HowMany) Gzrot(HowMany)]\n",
    "    \n",
    "    df_G_BC_on_knotpoints[\"G_xrot\"+str(i)] = df_Gx.loc[:,['velo']]\n",
    "    df_G_BC_on_knotpoints[\"G_yrot\"+str(i)] = df_Gy.loc[:,['velo']]\n",
    "    df_G_BC_on_knotpoints[\"G_zrot\"+str(i)] = df_Gz.loc[:,['velo']]\n",
    "    \n",
    "#df_G_BC_on_knotpoints\n",
    "\n",
    "\n",
    "#########STRAIN ###########\n",
    "\n",
    "    inputfile_strain_xrot = \"average_strain_BC_x_\"+str(f\"{i:03}\")+\"_RECTANGULAR.out\" # x-rot \n",
    "    inputfile_strain_yrot = \"average_strain_BC_y_\"+str(f\"{i:03}\")+\"_RECTANGULAR.out\" # y-rot \n",
    "    inputfile_strain_zrot = \"average_strain_BC_z_\"+str(f\"{i:03}\")+\"_RECTANGULAR.out\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "    df_xrot_strain=pd.read_csv(inputfile_strain_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot_strain=pd.read_csv(inputfile_strain_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot_strain=pd.read_csv(inputfile_strain_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "# CHANGE the column names \n",
    "    df_xrot_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    df_yrot_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "    df_zrot_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "\n",
    "# BUILD a column vector Gx (i)\n",
    "    df_xrot_exx = df_xrot_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints for strain\n",
    "    df_xrot_eyy = df_xrot_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints for strain\n",
    "    df_xrot_exy = df_xrot_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints for strain\n",
    "    \n",
    "    df_xrot_exx=df_xrot_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_xrot_eyy=df_xrot_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_xrot_exy=df_xrot_exy.rename(columns ={'exy': 'strain'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_exx=df_xrot_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_eyy=df_xrot_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_exy=df_xrot_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx_strain=[df_xrot_exx,df_xrot_eyy,df_xrot_exy]\n",
    "    df_Gx_strain=pd.concat(frames_Gx_strain,ignore_index=True) # merge the three dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "    df_yrot_exx = df_yrot_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints for strain\n",
    "    df_yrot_eyy = df_yrot_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints for strain\n",
    "    df_yrot_exy = df_yrot_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints for strain\n",
    "    \n",
    "    df_yrot_exx=df_yrot_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_yrot_eyy=df_yrot_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_yrot_exy=df_yrot_exy.rename(columns ={'exy': 'strain'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_exx=df_yrot_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_eyy=df_yrot_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_exy=df_yrot_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gy_strain=[df_yrot_exx,df_yrot_eyy,df_yrot_exy]\n",
    "    df_Gy_strain=pd.concat(frames_Gy_strain,ignore_index=True) # merge the three dataFrames into one    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "    df_zrot_exx = df_zrot_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints for strain\n",
    "    df_zrot_eyy = df_zrot_strain.iloc[:,[2,1,4]]  # saved eyy basis function on the midpoints for strain\n",
    "    df_zrot_exy = df_zrot_strain.iloc[:,[2,1,5]]  # saved exy basis function on the midpoints for strain\n",
    "    \n",
    "    df_zrot_exx=df_zrot_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "    df_zrot_eyy=df_zrot_eyy.rename(columns ={'eyy': 'strain'}) #column name change\n",
    "    df_zrot_exy=df_zrot_exy.rename(columns ={'exy': 'strain'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_exx=df_zrot_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_eyy=df_zrot_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_exy=df_zrot_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gz_strain=[df_zrot_exx,df_zrot_eyy,df_zrot_exy]\n",
    "    df_Gz_strain=pd.concat(frames_Gz_strain,ignore_index=True) # merge the three dataFrames into one \n",
    "    \n",
    "    \n",
    "    df_G_BC_on_midpoints_strain[\"G_xrot\"+str(i)] = df_Gx_strain.loc[:,['strain']]\n",
    "    df_G_BC_on_midpoints_strain[\"G_yrot\"+str(i)] = df_Gy_strain.loc[:,['strain']]\n",
    "    df_G_BC_on_midpoints_strain[\"G_zrot\"+str(i)] = df_Gz_strain.loc[:,['strain']]\n",
    "\n",
    "    \n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "#                         Horizontal continuous velocity model.                        #\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "frames_horizontal = [df_G_FT_on_knotpoints, df_G_BC_on_knotpoints]\n",
    "df_G_horizontal_continuous=pd.concat(frames_horizontal, axis=1) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "continuous_hor_model = df_G_horizontal_continuous.to_numpy() @ df_model1.to_numpy()\n",
    "knotpointnum=int(len(continuous_hor_model)/2)\n",
    "Xmodel=continuous_hor_model[0:knotpointnum,0]\n",
    "Ymodel=continuous_hor_model[knotpointnum:,0]\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "#                         Vertical continuous velocity model.                          #\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "df_G_vertical_continuous=df_G_FT_on_knotpoints_zzz\n",
    "\n",
    "continuous_ver_model = df_G_vertical_continuous.to_numpy() @ df_model1.to_numpy()[360*3:360*4,]\n",
    "Zmodel=continuous_ver_model[:,0]\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "#                         Horizontal continuous strain rate model.                     #\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "#     df_G_FT_on_midpoints_strain\n",
    "#     df_G_BC_on_midpoints_strain\n",
    "frames_horizontal_strain = [df_G_FT_on_midpoints_strain, df_G_BC_on_midpoints_strain]\n",
    "df_G_horizontal_continuous_strain=pd.concat(frames_horizontal_strain, axis=1) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "continuous_hor_model_strain = df_G_horizontal_continuous_strain.to_numpy() @ df_model1.to_numpy()\n",
    "midpointnum=int(len(continuous_hor_model_strain)/3)\n",
    "eXXmodel=continuous_hor_model_strain[0:midpointnum,0]\n",
    "eYYmodel=continuous_hor_model_strain[midpointnum:2*midpointnum,0]\n",
    "eXYmodel=continuous_hor_model_strain[2*midpointnum:3*midpointnum,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE continuous field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hori = df_zzz.loc[:,['lon','lat']]\n",
    "df_hori['ve'] = Xmodel\n",
    "df_hori['vn'] = Ymodel\n",
    "df_hori['se'] = np.zeros(len(Ymodel))\n",
    "df_hori['sn'] = np.zeros(len(Ymodel))\n",
    "df_hori['corr'] = np.zeros(len(Ymodel))\n",
    "\n",
    "df_vert = df_zzz.loc[:,['lon','lat']]\n",
    "df_vert['vz'] = Zmodel\n",
    "\n",
    "\n",
    "outputFILE_hori=\"vel_horizontal_cont_pred.gmt\"\n",
    "df_hori.to_csv(outputFILE_hori, header=None, index=None, sep=' ', float_format='%g')\n",
    "\n",
    "outputFILE_vert=\"vel_vertical_cont_pred.dat\"\n",
    "df_vert.to_csv(outputFILE_vert, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strain=df_xrot_exx.loc[:,['lon','lat']]\n",
    "df_strain['exx']=eXXmodel\n",
    "df_strain['eyy']=eYYmodel\n",
    "df_strain['exy']=eXYmodel\n",
    "df_strain['num']=range(len(eXXmodel))\n",
    "df_strain['sxx']=np.zeros((len(eXXmodel),))\n",
    "df_strain['syy']=np.zeros((len(eXXmodel),))\n",
    "df_strain['sxy']=np.zeros((len(eXXmodel),))\n",
    "df_strain_save = df_strain[['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']]\n",
    "\n",
    "outputFILE_strain=\"average_strain_cont_pred.out\"\n",
    "df_strain_save.to_csv(outputFILE_strain, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>exx</th>\n",
       "      <th>eyy</th>\n",
       "      <th>exy</th>\n",
       "      <th>sxx</th>\n",
       "      <th>syy</th>\n",
       "      <th>sxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33.225</td>\n",
       "      <td>-116.975</td>\n",
       "      <td>13.063322</td>\n",
       "      <td>-24.117722</td>\n",
       "      <td>8.067364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33.225</td>\n",
       "      <td>-116.925</td>\n",
       "      <td>11.838046</td>\n",
       "      <td>-29.433506</td>\n",
       "      <td>7.880877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33.225</td>\n",
       "      <td>-116.875</td>\n",
       "      <td>14.218176</td>\n",
       "      <td>-30.589871</td>\n",
       "      <td>12.781633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>33.225</td>\n",
       "      <td>-116.825</td>\n",
       "      <td>28.227386</td>\n",
       "      <td>-41.281095</td>\n",
       "      <td>17.449160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.225</td>\n",
       "      <td>-116.775</td>\n",
       "      <td>52.308496</td>\n",
       "      <td>-61.898864</td>\n",
       "      <td>16.230756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>355</td>\n",
       "      <td>34.075</td>\n",
       "      <td>-116.225</td>\n",
       "      <td>98.381494</td>\n",
       "      <td>-102.001155</td>\n",
       "      <td>-23.072646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>356</td>\n",
       "      <td>34.075</td>\n",
       "      <td>-116.175</td>\n",
       "      <td>92.577175</td>\n",
       "      <td>-77.847798</td>\n",
       "      <td>-4.198936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>357</td>\n",
       "      <td>34.075</td>\n",
       "      <td>-116.125</td>\n",
       "      <td>83.869890</td>\n",
       "      <td>-66.234192</td>\n",
       "      <td>-10.042404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>358</td>\n",
       "      <td>34.075</td>\n",
       "      <td>-116.075</td>\n",
       "      <td>68.006942</td>\n",
       "      <td>-62.938764</td>\n",
       "      <td>-3.434736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>359</td>\n",
       "      <td>34.075</td>\n",
       "      <td>-116.025</td>\n",
       "      <td>-30.009202</td>\n",
       "      <td>164.561295</td>\n",
       "      <td>-36.242026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num     lat      lon        exx         eyy        exy  sxx  syy  sxy\n",
       "0      0  33.225 -116.975  13.063322  -24.117722   8.067364  0.0  0.0  0.0\n",
       "1      1  33.225 -116.925  11.838046  -29.433506   7.880877  0.0  0.0  0.0\n",
       "2      2  33.225 -116.875  14.218176  -30.589871  12.781633  0.0  0.0  0.0\n",
       "3      3  33.225 -116.825  28.227386  -41.281095  17.449160  0.0  0.0  0.0\n",
       "4      4  33.225 -116.775  52.308496  -61.898864  16.230756  0.0  0.0  0.0\n",
       "..   ...     ...      ...        ...         ...        ...  ...  ...  ...\n",
       "355  355  34.075 -116.225  98.381494 -102.001155 -23.072646  0.0  0.0  0.0\n",
       "356  356  34.075 -116.175  92.577175  -77.847798  -4.198936  0.0  0.0  0.0\n",
       "357  357  34.075 -116.125  83.869890  -66.234192 -10.042404  0.0  0.0  0.0\n",
       "358  358  34.075 -116.075  68.006942  -62.938764  -3.434736  0.0  0.0  0.0\n",
       "359  359  34.075 -116.025 -30.009202  164.561295 -36.242026  0.0  0.0  0.0\n",
       "\n",
       "[360 rows x 9 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_strain_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
