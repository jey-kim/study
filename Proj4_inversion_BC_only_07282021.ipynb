{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover Boundary velocity (Least Square) \n",
    "> jeonghyeop kim (2021/7/28) <font color=red>(v. testing)</font>\n",
    "> \\\n",
    "> My first inversion code written in python\n",
    "\n",
    "This code uses:\n",
    "1. data vector: **`vel_on_boundary_wrt_URcorner.gmt`** & \\\n",
    "2. G-matrix components: **`vel_BC_%d_%d.gmt`**, (% 001..024; 100, 010, 001) \\\n",
    "          100 : x-rotation; 010 : y-rotation; 001 : z-rotation\n",
    "3. For **`vel_BC_%d_%d.gmt`**, I don't know how Ali generates them yet. Talk to him or Bill\n",
    "4. assumed that UCERF3 velocity is errorless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "pd.__version__\n",
    "#np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP1`: **BUILD a data vector,  $\\vec{d}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input file\n",
    "inputFILE = \"vel_on_boundary_wrt_URcorner.gmt\" # input file name\n",
    "df_input = pd.read_csv(inputFILE, header = None, sep =' ')\n",
    "df_input.columns = ['lon','lat','ve','vn','se','sn','corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a 'data' vector d along with coordinate information\n",
    "\n",
    "df_data_x = df_input.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y = df_input.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "\n",
    "df_data_x=df_data_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y=df_data_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "df_data_x['flag']=np.array([1] * len(df_data_x)) # 1 = velocity east-component\n",
    "df_data_y['flag']=np.array([2] * len(df_data_y)) # 2 = velocity north-component\n",
    "\n",
    "# !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "\n",
    "df_data_x=df_data_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y=df_data_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "\n",
    "# MERGE two columns (n*1) into a new column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "\n",
    "frames=[df_data_x,df_data_y]\n",
    "df_data=pd.concat(frames,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "df_data=df_data['velo']\n",
    "\n",
    "#df_data #to check the data vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:   \n",
    ">If your data have errors, \\\n",
    "build a **W** matrix in which its diagonal elements \\\n",
    "are of a vector made of 1/errors (in order).  \\\n",
    "Your new data vector **d<sub>w</sub>** = **W** **d**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP2`: **BUILD G-Matrix, $\\bar{\\bar{G}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on the read_csv(sep option) below: \\\n",
    "> **`'(?:,|\\s+)'`** - is a RegEx for selecting either comma or any number of consecutive spaces/tabs \\\n",
    "> See the answer at StackOverFlow [click here](https://stackoverflow.com/questions/43784422/pandas-error-when-reading-csv-file-using-sep-and-comment-arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many files do you have for the boudnary basis functions? :\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 24\n"
     ]
    }
   ],
   "source": [
    "df_G = pd.DataFrame(index = range(len(df_data))) \n",
    "# Make a blank G matrix\n",
    "\n",
    "\n",
    "print('How many files do you have for the boudnary basis functions? :')\n",
    "HowMany = input()\n",
    "HowMany = int(HowMany)\n",
    "#24 for the boundary condition (7/28/2021)\n",
    "\n",
    "for i in range(1,HowMany+1): # i in range(0+1,how many files+1?); i in {001 002 003 ... 999} \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_\"+str(f\"{i:03}\")+\"_100.gmt\" # x-rot : 100\n",
    "    inputfile_yrot = \"vel_BC_\"+str(f\"{i:03}\")+\"_010.gmt\" # y-rot : 010\n",
    "    inputfile_zrot = \"vel_BC_\"+str(f\"{i:03}\")+\"_001.gmt\" # z-rot : 001\n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_xrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_yrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gx (i)\n",
    "\n",
    "    df_xrot_x = df_xrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_xrot_y = df_xrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_xrot_x=df_xrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_xrot_y=df_xrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    df_xrot_x['flag']=np.array([1] * len(df_xrot_x)) # 1 = velocity east-component\n",
    "    df_xrot_y['flag']=np.array([2] * len(df_xrot_y)) # 2 = velocity north-component\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_x=df_xrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_y=df_xrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx=[df_xrot_x,df_xrot_y]\n",
    "    df_Gx=pd.concat(frames_Gx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "\n",
    "    df_yrot_x = df_yrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_yrot_y = df_yrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_yrot_x=df_yrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_yrot_y=df_yrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    df_yrot_x['flag']=np.array([1] * len(df_yrot_x)) # 1 = velocity east-component\n",
    "    df_yrot_y['flag']=np.array([2] * len(df_yrot_y)) # 2 = velocity north-component\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_x=df_yrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_y=df_yrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gy=[df_yrot_x,df_yrot_y]\n",
    "    df_Gy=pd.concat(frames_Gy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "\n",
    "    df_zrot_x = df_zrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_zrot_y = df_zrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_zrot_x=df_zrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_zrot_y=df_zrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    df_zrot_x['flag']=np.array([1] * len(df_zrot_x)) # 1 = velocity east-component\n",
    "    df_zrot_y['flag']=np.array([2] * len(df_zrot_y)) # 2 = velocity north-component\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_x=df_zrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_y=df_zrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gz=[df_zrot_x,df_zrot_y]\n",
    "    df_Gz=pd.concat(frames_Gz,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# SAVE G-matrix\n",
    "# Gmatrix = [Gxrot(1) Gyrot(1) Gzrot(1) ... Gxrot(HowMany) Gyrot(HowMany) Gzrot(HowMany)]\n",
    "    \n",
    "    df_G[\"G_x\"+str(i)] = df_Gx.loc[:,['velo']]\n",
    "    df_G[\"G_y\"+str(i)] = df_Gy.loc[:,['velo']]\n",
    "    df_G[\"G_z\"+str(i)] = df_Gz.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the G-matrix\n",
    "# outputFILE='Gmatrix_test.dat'\n",
    "# df_G.to_csv(outputFILE, header=None, index=None, sep=' ',float_format='%f6.3')\n",
    "# df_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `STEP3`: Inversion (Least Square Sense) \n",
    ">  **d** = **G** **m** \\\n",
    ">  **G'** **d** = **G'** **G** **m** \\\n",
    ">  (**G'** **G**)<sup>-1</sup> **G'** **d** = $\\hat{m}$\n",
    "\n",
    " One can try <font color=red>weighting</font> and <font color=red>damping</font> later! \\\n",
    " For now (7/28/2021), this code just applies a simplest LSM. \n",
    " \n",
    "**Pandas: For element-wise operation syntax,** [click this](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.multiply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G'\n",
    "df_G_prime = df_G.transpose() \n",
    "\n",
    "# G'G\n",
    "# >Two different ways to compute a matrix multiplication\n",
    "# >1st method\n",
    "GpG1=df_G_prime.dot(df_G) #G'G\n",
    "# >2nd method\n",
    "GpG2=df_G_prime @ df_G #G'G\n",
    "# >These results are same.\n",
    "# >Let's take the second one as G'G\n",
    "GpG = GpG2 #GpG is G'G\n",
    "\n",
    "\n",
    "# inv(G'G)\n",
    "# > Two different ways to obtain inverse matrix\n",
    "# > 1st method: np.linalg.inv \n",
    "df_inv_GpG = pd.DataFrame(np.linalg.inv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "# > 2nd method: np.linalg.pinv (Moore-Penrose inverse (SVD))\n",
    "df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "\n",
    "\n",
    "# inv(G'G)*G'*d = model(LSM)\n",
    "df_model1=df_inv_GpG@df_G_prime@df_data\n",
    "df_model2=df_pinv_GpG@df_G_prime@df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G_x1    -0.001435\n",
       "G_y1    -0.002874\n",
       "G_z1     0.002163\n",
       "G_x2    -0.001825\n",
       "G_y2    -0.003654\n",
       "           ...   \n",
       "G_y23    0.000003\n",
       "G_z23   -0.000002\n",
       "G_x24    0.000052\n",
       "G_y24    0.000103\n",
       "G_z24   -0.000078\n",
       "Length: 72, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The difference is very small \n",
    "df_model1-df_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_predicted = df_G@df_model1 #data predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03189875729010651"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#norm2 misfit\n",
    "df_norm2=(df_data-df_data_predicted)**2\n",
    "df_norm2=df_norm2.to_numpy().sum()\n",
    "df_norm2=np.lib.scimath.sqrt(df_norm2)\n",
    "df_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE predicted velocity values on the boundary in *.gmt file.\n",
    "\n",
    "num_velo_point=len(df_data_predicted)/2\n",
    "num_velo_point=int(num_velo_point)\n",
    "df_prediction_vx=df_data_predicted.iloc[0:num_velo_point] #vx predicted\n",
    "df_prediction_vy=df_data_predicted.iloc[num_velo_point:2*num_velo_point] #vy predicted\n",
    "df_prediction_vy=df_prediction_vy.reset_index(drop=True) #index starts from 0, and drop old index numbers\n",
    "\n",
    "\n",
    "# Use 'df_data_x' (**already sorted by lat and lon**) as a frame to save the predicted values\n",
    "# 'df_data_x' has columns : lon lat vel flag\n",
    "df_save = df_data_x \n",
    "df_save['ve'] = df_prediction_vx #append a new column named ve\n",
    "df_save['vn'] = df_prediction_vy #append a new column named vn\n",
    "df_save['se'] = np.array([0] * len(df_save)) #append a new column named se : zero vector\n",
    "df_save['sn'] = np.array([0] * len(df_save)) #append a new column named sn : zero vector\n",
    "df_save['corr'] = np.array([0] * len(df_save)) #append a new column named corr : zero vector\n",
    "df_save=df_save.loc[:,['lon','lat','ve','vn','se','sn','corr']] #slice the seven columns of interest!\n",
    "\n",
    "\n",
    "#Save output : \"vel_on_boundary_wrt_URcorner_pred.gmt\"\n",
    "outputFILE=\"vel_on_boundary_wrt_URcorner_pred.gmt\"\n",
    "df_save.to_csv(outputFILE, header=None, index=None, sep=' ',float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "psbasemap [INFORMATION]: Constructing the basemap\n",
      "psbasemap [INFORMATION]: Central meridian not given, default to -116.61\n",
      "psbasemap [INFORMATION]: Map scale is 1.23688 km per cm or 1:123688.\n",
      "pscoast [INFORMATION]: Central meridian not given, default to -116.61\n",
      "pscoast [INFORMATION]: Map scale is 1.23688 km per cm or 1:123688.\n",
      "pscoast [INFORMATION]: Selected ice front line as Antarctica coastline\n",
      "pscoast [INFORMATION]: GSHHG version 2.3.7\n",
      "pscoast [INFORMATION]: Derived from World Vector Shoreline, CIA WDB-II, and Atlas of the Cryosphere\n",
      "pscoast [INFORMATION]: Processed by Paul Wessel and Walter H. F. Smith, 1994-2017\n",
      "pscoast [INFORMATION]: Adding Borders...psxy [INFORMATION]: Processing input table data\n",
      "psxy [INFORMATION]: Central meridian not given, default to -116.61\n",
      "psxy [INFORMATION]: Map scale is 1.23688 km per cm or 1:123688.\n",
      "psxy [INFORMATION]: Reading Data Table from Stream 7fff889fbe50\n",
      "psxy [INFORMATION]: Plotting segment 0\n",
      "pstext [INFORMATION]: Processing input text table data\n",
      "pstext [INFORMATION]: Central meridian not given, default to -116.61\n",
      "pstext [INFORMATION]: Map scale is 1.23688 km per cm or 1:123688.\n",
      "pstext [INFORMATION]: Reading Data Table from Standard Input stream\n",
      "pstext [INFORMATION]: pstext: Plotted 1 text strings\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Plot!\n",
    "#psvelo (GMT4 syntax) -Alinethickness/?/arrowthickness\n",
    "sh image_boundary_data_wrt_URC_obs_pred\n",
    "gmt psconvert boundary_data_wrt_URC_pred.ps -Tf -A\n",
    "open boundary_data_wrt_URC_pred.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
