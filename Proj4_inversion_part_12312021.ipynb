{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figs/GEOS_logo.pdf\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert BC, InSAR and GNSS data (weighted and damped LSM): \n",
    "## <font color=blue>\"inversion_part.ipynb\"</font>\n",
    "#### Dec 31, 2021  <font color=red>(v. testing)</font>\n",
    "##### Jeonghyeop Kim (jeonghyeop.kim@gmail.com)\n",
    "\n",
    "1. This code is a part of the joint inversion project (project4: joint inversion of GNSS and InSAR)\n",
    "2. The G-matrix will be loaded from a file which was generated in the previous step\n",
    "3. Damped and Weighted LSM will be performed to find the best coefficients of the basis functions\n",
    "4. The goal is to find the best linear combination of the basis functions that predicted the data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########READ Weighting and Damping parameters\n",
    "\n",
    "# weight_for_InSAR = sys.argv[1]\n",
    "# weight_for_InSAR = float(weight_for_InSAR)\n",
    "\n",
    "# weight_for_GNSS = sys.argv[2]\n",
    "# weight_for_GNSS = float(weight_for_GNSS)\n",
    "# weight_for_BC = weight_for_GNSS \n",
    "# #BC velocity is from GNSS data\n",
    "\n",
    "# damping_for_horizontal = sys.argv[3]\n",
    "# damping_for_horizontal = float(damping_for_horizontal)\n",
    "# damping_for_rotation = damping_for_horizontal \n",
    "# # Rotations do NOT contribute the vertical field\n",
    "\n",
    "# damping_for_vertical = sys.argv[4]\n",
    "# damping_for_vertical = float(damping_for_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_InSAR=5\n",
    "weight_for_GNSS=1\n",
    "weight_for_BC = weight_for_GNSS\n",
    "damping_for_horizontal=1\n",
    "damping_for_rotation = damping_for_horizontal\n",
    "damping_for_vertical=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## How Many Basis Functions (FT #ofCells; BC #ofRotations)\n",
    "HowManyBasisFunctions=np.loadtxt(\"HowMany.txt\")\n",
    "HowManyCell=int(HowManyBasisFunctions[1])\n",
    "HowManyRot=int(HowManyBasisFunctions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmatrix_file=\"G_matrix.out\"\n",
    "df_G_final=pd.read_csv(Gmatrix_file, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CHOOSE THE TYPE OF LSMs for the solution\n",
    "\n",
    "inversion_flag = 4 \n",
    "\n",
    "# 1 is simple LSM\n",
    "# 2 is Pseudo LSM \n",
    "# 3 is Damped LSM (Ridge Regularization)\n",
    "\n",
    "################## Testing ###########################\n",
    "# 4 is Damped LSM (L-1 Norm : LASSO [scikit-learn])  #\n",
    "# 5 is Damped LSM (L-1 Norm : STLSQ )                #\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "outputFILE_GNSS_XandY=\"vel_GNSS_pred.gmt\" #Prediction for GNSS\n",
    "outputFILE_D=\"dsd_InSAR_pred.dat\" #Prediction for (Descending) InSAR\n",
    "outputFILE_A=\"asd_InSAR_pred.dat\" #Prediction for (Ascending) InSAR\n",
    "outputFILE_BC_XandY=\"vel_BC_pred.dat\" #Prediction for BC velocity field\n",
    "outputFILE_model=\"model_coef.dat\" # LSM model coefficients\n",
    "outputFILE_hori=\"vel_horizontal_cont_pred.gmt\" #Continuous horizontal field\n",
    "outputFILE_vert=\"vel_vertical_cont_pred.dat\" #Continuous vertical field \n",
    "outputFILE_strain=\"average_strain_cont_pred.out\" #Continuous Strain rate field (midpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `STEP 1:` **BUILD a data vector,  $\\vec{d}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input files\n",
    "\n",
    "# 1. Boundary velocity (on the boundary)\n",
    "inputBC = \"BC_input.gmt\"  # Boundary Condition Velocity [mm/yr]\n",
    "df_inputBC = pd.read_csv(inputBC, header = None, sep =' ')\n",
    "df_inputBC.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputBC.loc[:,['ve']] = df_inputBC.loc[:,['ve']]  \n",
    "df_inputBC.loc[:,['vn']] = df_inputBC.loc[:,['vn']]\n",
    "\n",
    "# 2. GNSS data\n",
    "inputGNSS = \"GNSS_input.gmt\"  # GNSS [mm/yr]\n",
    "df_inputGNSS = pd.read_csv(inputGNSS, header = None, sep=r'(?:,|\\s+)', comment='#', engine='python')\n",
    "df_inputGNSS.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputGNSS.loc[:,['ve']] = df_inputGNSS.loc[:,['ve']] \n",
    "df_inputGNSS.loc[:,['vn']] = df_inputGNSS.loc[:,['vn']]\n",
    "\n",
    "# 2. InSAR Descending \n",
    "inputInSAR_D = \"dsd_InSAR_input.dat\" # InSAR DECS [mm/yr]\n",
    "df_inputInSAR_D = pd.read_csv(inputInSAR_D, header = None, sep = ' ')\n",
    "df_inputInSAR_D.columns = ['lon','lat','velo','Px','Py','Pz']  \n",
    "df_inputInSAR_D.loc[:,['velo']] = df_inputInSAR_D.loc[:,['velo']]\n",
    "\n",
    "# 3. InSAR Ascending \n",
    "inputInSAR_A = \"asd_InSAR_input.dat\" # InSAR ASCE [mm/yr]\n",
    "df_inputInSAR_A = pd.read_csv(inputInSAR_A, header = None, sep = ' ')\n",
    "df_inputInSAR_A.columns = ['lon','lat','velo','Px','Py','Pz'] \n",
    "df_inputInSAR_A.loc[:,['velo']] = df_inputInSAR_A.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>NOTE: the pointing vectors are from the perspective of the ground! NOT of the satellite </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################         GNSS         ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD GNSS data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the GNSS velocity.\n",
    "df_data_x_GNSS = df_inputGNSS.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y_GNSS = df_inputGNSS.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "df_data_x_GNSS=df_data_x_GNSS.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y_GNSS=df_data_y_GNSS.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_data_x_GNSS=df_data_x_GNSS.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y_GNSS=df_data_y_GNSS.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesGNSS=[df_data_x_GNSS,df_data_y_GNSS]\n",
    "df_data_GNSS_all=pd.concat(framesGNSS,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "# SAVE GNSS velocity separately\n",
    "df_data_GNSS=df_data_GNSS_all.loc[:,['velo']]\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################       Boundary       ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD a boundary condition data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the velocity.\n",
    "df_data_x_BC = df_inputBC.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y_BC = df_inputBC.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "df_data_x_BC=df_data_x_BC.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y_BC=df_data_y_BC.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT_VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_data_x_BC=df_data_x_BC.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y_BC=df_data_y_BC.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesBC=[df_data_x_BC,df_data_y_BC]\n",
    "df_data_BC_all=pd.concat(framesBC,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "# SAVE BC velocity separately\n",
    "df_data_BC=df_data_BC_all.loc[:,['velo']]\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "######################################################        InSAR         ######################################################    \n",
    "##################################################################################################################################\n",
    "##################################################################################################################################\n",
    "\n",
    "# BUILD a InSAR data vector along with coordinate information.\n",
    "# The rows of the InSAR data vector is in the order of descending-orbit data, and ascending-orbit data. \n",
    "# Track the pointing vector values together with the rate data for the G-matrix.\n",
    "\n",
    "# !! SORT_VALUES !! # lat (ascending) first, and then lon (ascending).\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "df_inputInSAR_D=df_inputInSAR_D.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR_A=df_inputInSAR_A.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*2) into a column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "framesInSAR=[df_inputInSAR_D,df_inputInSAR_A]\n",
    "df_data_InSAR_all=pd.concat(framesInSAR,ignore_index=True) # merge the four dataFrames into one\n",
    "\n",
    "# SAVE InSAR velocity separately\n",
    "df_data_InSAR = df_data_InSAR_all.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GNSS, InSAR, and BC data vectors into the final data vector\n",
    "framesFinal = [df_data_GNSS, df_data_InSAR, df_data_BC]\n",
    "df_data_total = pd.concat(framesFinal,ignore_index=True) # merge the two dataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_total.columns = ['data'] # DATA VECTOR [[GNSSx], [GNSSy], [InSAR_D], [InSAR_A], [BCx], [BCy]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_data_total)!=len(df_G_final):\n",
    "    print(\"WARNING: Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 3:` RUN Joint Inversion (LSM)\n",
    "> G-matrix = **df_G_final** \\\n",
    "> data vec = **df_data_total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Diagonal Weighting Matrix W\n",
    "nGNSS=len(df_data_GNSS)\n",
    "nBC=len(df_data_BC)\n",
    "nInSAR=len(df_data_InSAR)\n",
    "nTotal=len(df_data_total)\n",
    "\n",
    "errorGNSS = np.ones(nGNSS)*weight_for_GNSS\n",
    "errorInSAR = np.ones(nInSAR)*weight_for_InSAR \n",
    "errorBC = np.ones(nBC)*weight_for_BC\n",
    "\n",
    "errorTotal = np.concatenate((errorGNSS,errorInSAR, errorBC),axis=0)\n",
    "errorTotalinv = 1/errorTotal\n",
    "W = np.diag(errorTotalinv)\n",
    "\n",
    "# convert into a dataframe\n",
    "dfW = pd.DataFrame(W)\n",
    "\n",
    "# When calculating predictions, the non-weighted G-matrix is needed. \n",
    "# SAVE it!\n",
    "df_G_final_save = df_G_final\n",
    "\n",
    "# When calculating the misfit, the non-weighted data is needed. \n",
    "# SAVE it!\n",
    "df_data_total_save = df_data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the Diagonal Weighting Matrix dfW to the data vector and Gmatrix\n",
    "\n",
    "df_G_final = dfW @ df_G_final_save\n",
    "df_data_total = dfW @ df_data_total_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_flag=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (5988,) could not be broadcast to indexing result of shape (5988,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kt/zdsy5jqj2tn2lbh6qhjxdlgh0000gn/T/ipykernel_1298/1069439798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mdf_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_L1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m###### s_L1 to pandas Df named 'df_model1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;31m# actually do the set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setitem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;31m# We are setting _all_ of the array's values, so can cast to new dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_ea_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (5988,) could not be broadcast to indexing result of shape (5988,1)"
     ]
    }
   ],
   "source": [
    "df_G_prime = df_G_final.transpose() \n",
    "\n",
    "# G'G\n",
    "# >Two different ways to compute a matrix multiplication\n",
    "# >1st method\n",
    "GpG1=df_G_prime.dot(df_G_final) #G'G\n",
    "# >2nd method\n",
    "GpG2=df_G_prime @ df_G_final #G'G\n",
    "# >These results are same.\n",
    "# >Let's take the second one as G'G\n",
    "GpG = GpG2 #GpG is G'G\n",
    "# inv(G'G)\n",
    "# > Two different ways to obtain inverse matrix\n",
    "# > 1st method: np.linalg.inv \n",
    "\n",
    "\n",
    "###############################\n",
    "## inv(G'G)*G'*d = model(LSM) #\n",
    "###############################\n",
    "if inversion_flag == 1:\n",
    "    #  1st method: np.linalg.inv\n",
    "    df_inv_GpG = pd.DataFrame(np.linalg.inv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_inv_GpG@df_G_prime@df_data_total #inversion\n",
    "    \n",
    "elif inversion_flag == 2:\n",
    "    #  2nd method: np.linalg.pinv (Moore-Penrose inverse (SVD))\n",
    "    df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion\n",
    "    \n",
    "elif inversion_flag == 3:\n",
    "    #  3rd method: Damping (Tikhonov Regularization)\n",
    "    alp=damping_for_horizontal*np.ones((HowManyCell*3,1))**2 # damping parameter for hori\n",
    "    bet=damping_for_vertical*np.ones((HowManyCell,1))**2 # damping parameter for vert\n",
    "    gam=damping_for_rotation*np.ones((HowManyRot*3,1))**2 # damping parameter for rot\n",
    "    array_tuple = (alp, bet, gam)\n",
    "    lamb = np.vstack(array_tuple) \n",
    "    lamb = lamb[:,0]\n",
    "    damping_matrix=np.diag(lamb) # a*a*I    \n",
    "    GpG_damping = GpG + damping_matrix #(G'G + a*a*I)\n",
    "    GpD=df_G_prime@df_data_total\n",
    "    df_model_damping= np.linalg.solve(GpG_damping,GpD)\n",
    "    df_model1=pd.DataFrame(df_model_damping[:,0])\n",
    "    df_model1.index=df_G_final_save.columns.values  \n",
    "\n",
    "    \n",
    "    \n",
    "elif inversion_flag == 4: \n",
    "#  4th method [Testing]: scikit-learn LASSO (L1-norm)\n",
    "\n",
    "    # For the Frame\n",
    "    df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion    \n",
    "    \n",
    "    np_G_final = df_G_final.to_numpy()\n",
    "    np_data_total= df_data_total.to_numpy()\n",
    "    np_data_total = np.squeeze(np_data_total)\n",
    "    from sklearn import linear_model\n",
    "    from sklearn import model_selection\n",
    "    reg = linear_model.LassoCV(cv=10).fit(np_G_final, np_data_total)\n",
    "    lasso = linear_model.Lasso(random_state=0, max_iter=10000)\n",
    "    alphas = np.logspace(-4, -0.5, 30)\n",
    "    tuned_parameters = [{'alpha': alphas}] \n",
    "    clf = model_selection.GridSearchCV(lasso, tuned_parameters, cv=10, refit=False)\n",
    "    clf.fit(np_G_final, np_data_total)\n",
    "    XL1 = linear_model.Lasso(alpha=clf.best_params_['alpha'])\n",
    "    XL1.fit(np_G_final, np_data_total)\n",
    "    s_L1 = XL1.coef_\n",
    "\n",
    "    a=s_L1.reshape(len(s_L1),1)\n",
    "    df_model1.loc[:,['data']]=a\n",
    "    ###### s_L1 to pandas Df named 'df_model1'\n",
    "    \n",
    "    \n",
    "else:\n",
    "    #  5th method [Testing]: STLSQ method (L1-norm)\n",
    "    # Started from L2-norm\n",
    "    alp=3*np.ones((HowManyCell*3,1))**2 # damping parameter for hori\n",
    "    bet=3*np.ones((HowManyCell,1))**2 # damping parameter for vert\n",
    "    gam=3*np.ones((HowManyRot*3,1))**2 # damping parameter for rot\n",
    "    array_tuple = (alp, bet, gam)\n",
    "    lamb = np.vstack(array_tuple) \n",
    "    lamb = lamb[:,0]\n",
    "    damping_matrix=np.diag(lamb) # a*a*I    \n",
    "    GpG_damping = GpG + damping_matrix #(G'G + a*a*I)\n",
    "    GpD=df_G_prime@df_data_total\n",
    "    df_model_damping= np.linalg.solve(GpG_damping,GpD)\n",
    "    df_model1=pd.DataFrame(df_model_damping[:,0])\n",
    "    df_model1.index=df_G_final_save.columns.values  \n",
    "\n",
    "    # STLSQ \n",
    "    x0 = df_model1.to_numpy() #initialize with L2 solution\n",
    "    from scipy.optimize import minimize\n",
    "    def L1_norm(x):\n",
    "        return np.linalg.norm(x,ord=1) \n",
    "    constr = ({'type' : 'eq', 'fun' : lambda x: df_G_final @ x - df_data_total})\n",
    "    res = minimize(L1_norm, x0, method = 'SLSQP', constraints = constr)\n",
    "    s_L1 = res.x\n",
    " \n",
    "    df_model1.loc[:,['data']]=s_L1\n",
    "    ###### s_L1 to pandas Df named 'df_model1'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_data_predicted = df_G_final_save @ df_model1\n",
    "# 'df_G_final_save' is the non-weighted G-matrix\n",
    "\n",
    "# squared norm2 misfit\n",
    "df_norm2=(df_data_total_save.to_numpy()-df_data_predicted.to_numpy())**2\n",
    "df_norm2=df_norm2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the predicted data vector into\n",
    "# (1) GNSS x and y\n",
    "# (2) InSAR 1\n",
    "# (3) InSAR 2\n",
    "# (4) BC vel x and y\n",
    "\n",
    "num_velo_pointGNSS=len(df_data_GNSS_all) # GNSS\n",
    "num_velo_pointGNSS=int(num_velo_pointGNSS)\n",
    "\n",
    "num_velo_pointInSAR=len(df_data_InSAR_all)/2  # 2 InSAR data sets in a column vector\n",
    "num_velo_pointInSAR=int(num_velo_pointInSAR)\n",
    "\n",
    "num_velo_pointBC=len(df_data_BC_all) #BC vel\n",
    "num_velo_pointBC=int(num_velo_pointBC)\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "#####  GNSS  #######\n",
    "####################\n",
    "\n",
    "#GNSS predicted x\n",
    "df_prediction_GNSS_x=df_data_predicted.iloc[0:int(num_velo_pointGNSS/2)] \n",
    "df_prediction_GNSS_x=df_prediction_GNSS_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_GNSS_y=df_data_predicted.iloc[int(num_velo_pointGNSS/2):num_velo_pointGNSS] \n",
    "df_prediction_GNSS_y=df_prediction_GNSS_y.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_GNSS_save = df_data_GNSS_all.iloc[0:int(num_velo_pointGNSS/2),[0,1]]\n",
    "\n",
    "df_save_GNSS_XandY = df_GNSS_save.reset_index(drop=True)\n",
    "df_save_GNSS_XandY['vx'] = df_prediction_GNSS_x\n",
    "df_save_GNSS_XandY['vn'] = df_prediction_GNSS_y\n",
    "df_save_GNSS_XandY['se'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['sn'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "df_save_GNSS_XandY['corr'] = np.zeros(len(df_prediction_GNSS_y))\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#####  INSAR  #######\n",
    "#####################\n",
    "\n",
    "df_prediction_D=df_data_predicted.iloc[num_velo_pointGNSS:num_velo_pointGNSS+num_velo_pointInSAR] #dLOS Descending\n",
    "df_prediction_D=df_prediction_D.reset_index(drop=True)\n",
    "\n",
    "df_prediction_A=df_data_predicted.iloc[num_velo_pointGNSS+num_velo_pointInSAR:num_velo_pointGNSS+2*num_velo_pointInSAR] #dLOS Ascending\n",
    "df_prediction_A=df_prediction_A.reset_index(drop=True)\n",
    "\n",
    "# Coordinate Template To Save Predicted InSAR Data\n",
    "df_InSAR_save = df_inputInSAR_D.iloc[:,[0,1]]   \n",
    "\n",
    "df_save_D = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_D['dLOS'] = df_prediction_D #append predicted dLOS Descending\n",
    "df_save_D['dLOS'] = df_save_D['dLOS']\n",
    "\n",
    "df_save_A = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A['dLOS'] = df_prediction_A #append predicted dLOS Ascending\n",
    "df_save_A['dLOS'] = df_save_A['dLOS']\n",
    "\n",
    "########################\n",
    "#####  Boundary ########\n",
    "########################\n",
    "\n",
    "#BC predicted x\n",
    "df_prediction_BC_x=df_data_predicted.iloc[num_velo_pointGNSS+2*num_velo_pointInSAR:num_velo_pointGNSS+2*num_velo_pointInSAR+int(num_velo_pointBC/2)] \n",
    "df_prediction_BC_x=df_prediction_BC_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_BC_y=df_data_predicted.iloc[num_velo_pointGNSS+2*num_velo_pointInSAR+int(num_velo_pointBC/2):num_velo_pointGNSS+2*num_velo_pointInSAR+num_velo_pointBC] \n",
    "df_prediction_BC_y=df_prediction_BC_y.reset_index(drop=True)\n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_BC_save = df_data_BC_all.iloc[0:int(num_velo_pointBC/2),[0,1]]\n",
    "df_save_BC_XandY = df_BC_save.reset_index(drop=True)\n",
    "df_save_BC_XandY['vx'] = df_prediction_BC_x\n",
    "df_save_BC_XandY['vn'] = df_prediction_BC_y\n",
    "df_save_BC_XandY['se'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['sn'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['corr'] = np.zeros(len(df_prediction_BC_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE predicted data sets\n",
    "df_save_GNSS_XandY.to_csv(outputFILE_GNSS_XandY, header=None, index=None, sep=' ', float_format='%g')\n",
    "df_save_D.to_csv(outputFILE_D, header=None, index=None, sep=' ',float_format='%g')\n",
    "df_save_A.to_csv(outputFILE_A, header=None, index=None, sep=' ',float_format='%g')\n",
    "df_save_BC_XandY.to_csv(outputFILE_BC_XandY, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE model coefficients\n",
    "df_model1.to_csv(outputFILE_model, header=None, index=None, float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-square statistics is : 74289.880884 [mm/yr]**2\n"
     ]
    }
   ],
   "source": [
    "print(\"chi-square statistics is : %f [mm/yr]**2\"  % df_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<div class=\"alert--icon\"> <i class=\"far fa-times-circle\"></i> </div>\n",
    "    <p> Weighted Tikhonov Regularization works pretty well. </p>\n",
    "    <b> But this approach cannot resolve short-wavelength features. </b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> TRY the L1 regularization. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 4:` Obtain 3-D continuous surface velocity & horizontal strain field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmatrix_file_cont_hori=\"G_matrix_cont_hori.out\"\n",
    "Gmatrix_file_cont_vert=\"G_matrix_cont_vert.out\"\n",
    "Gmatrix_file_strain=\"G_matrix_strain.out\"\n",
    "\n",
    "df_G_horizontal_continuous=pd.read_csv(Gmatrix_file_cont_hori, sep = ',')\n",
    "df_G_vertical_continuous=pd.read_csv(Gmatrix_file_cont_vert, sep = ',')\n",
    "df_G_horizontal_continuous_strain=pd.read_csv(Gmatrix_file_strain, sep = ',')\n",
    "########################################################################################\n",
    "#                         Horizontal continuous velocity model.                        #\n",
    "########################################################################################\n",
    "\n",
    "continuous_hor_model = df_G_horizontal_continuous.to_numpy() @ df_model1.to_numpy()\n",
    "continuous_num=int(len(continuous_hor_model)/2)\n",
    "Xmodel=continuous_hor_model[0:continuous_num,0]\n",
    "Ymodel=continuous_hor_model[continuous_num:,0]\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#                         Vertical continuous velocity model.                          #\n",
    "########################################################################################\n",
    "\n",
    "continuous_ver_model = df_G_vertical_continuous.to_numpy() @ df_model1.to_numpy()[HowManyCell*3:HowManyCell*4,]\n",
    "Zmodel=continuous_ver_model[:,0]\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#                         Horizontal continuous strain rate model.                     #\n",
    "########################################################################################\n",
    "\n",
    "continuous_hor_model_strain = df_G_horizontal_continuous_strain.to_numpy() @ df_model1.to_numpy()\n",
    "midpointnum=int(len(continuous_hor_model_strain)/3)\n",
    "eXXmodel=continuous_hor_model_strain[0:midpointnum,0]\n",
    "eYYmodel=continuous_hor_model_strain[midpointnum:2*midpointnum,0]\n",
    "eXYmodel=continuous_hor_model_strain[2*midpointnum:3*midpointnum,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE continuous 3-D velocity field.\n",
    "inputfile_zzz_continuous = \"./basis_functions_FT/vel_vert_FT_1_4_continuous.gmt\"\n",
    "df_zzz_continuous=pd.read_csv(inputfile_zzz_continuous, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "df_zzz_continuous.columns = ['lon','lat','vz']\n",
    "df_zzz_continuous=df_zzz_continuous.sort_values(['lat','lon'], ascending=[True, True])\n",
    "df_zzz_continuous=df_zzz_continuous.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_hori = df_zzz_continuous.loc[:,['lon','lat']]\n",
    "df_hori['ve'] = Xmodel\n",
    "df_hori['vn'] = Ymodel\n",
    "df_hori['se'] = np.zeros(len(Ymodel))\n",
    "df_hori['sn'] = np.zeros(len(Ymodel))\n",
    "df_hori['corr'] = np.zeros(len(Ymodel))\n",
    "\n",
    "df_vert = df_zzz_continuous.loc[:,['lon','lat']]\n",
    "df_vert['vz'] = Zmodel\n",
    "\n",
    "df_hori.to_csv(outputFILE_hori, header=None, index=None, sep=' ', float_format='%g')\n",
    "df_vert.to_csv(outputFILE_vert, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE horizontal strain rate field.\n",
    "\n",
    "inputfile_strain_xrot = \"./basis_functions_BC/average_strain_BC_x_001_RECTANGULAR.out\" # x-rot \n",
    "df_xrot_strain=pd.read_csv(inputfile_strain_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "df_xrot_strain.columns = ['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']\n",
    "df_xrot_exx = df_xrot_strain.iloc[:,[2,1,3]]  # saved exx basis function on the midpoints for strain\n",
    "df_xrot_exx=df_xrot_exx.rename(columns ={'exx': 'strain'}) #column name change\n",
    "df_xrot_exx=df_xrot_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "\n",
    "\n",
    "df_strain=df_xrot_exx.loc[:,['lon','lat']]\n",
    "df_strain['exx']=eXXmodel\n",
    "df_strain['eyy']=eYYmodel\n",
    "df_strain['exy']=eXYmodel\n",
    "df_strain['num']=range(len(eXXmodel))\n",
    "df_strain['sxx']=np.zeros((len(eXXmodel),))\n",
    "df_strain['syy']=np.zeros((len(eXXmodel),))\n",
    "df_strain['sxy']=np.zeros((len(eXXmodel),))\n",
    "df_strain_save = df_strain[['num','lat','lon','exx','eyy','exy','sxx','syy','sxy']]\n",
    "\n",
    "df_strain_save.to_csv(outputFILE_strain, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
