{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figs/GEOS_logo.pdf\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert BC and InSAR signal (weighted LSM): <font color=blue>\"inversion_BC_and_InSAR_weighting.ipynb\"</font>\n",
    "#### Oct 13, 2021  <font color=red>(v. testing)</font>\n",
    "##### Jeonghyeop Kim (jeonghyeop.kim@gmail.com)\n",
    "\n",
    "\n",
    ">  input files :  \\\n",
    "> <font color=red> **Basis functions related to force terms** </font> \\\n",
    "> **`vel_hori_FT_i_j_on_InSAR.gmt`**, **`vel_hori_FT_i_j_on_boundary.gmt`** \\\n",
    "> **`vel_vert_FT_i_j_on_InSAR.gmt`**, **`vel_vert_FT_i_j_on_boundary.gmt`** \\\n",
    "> **`<i = 1..140 & j = 1..4>`** \\\n",
    "> <font color=red> **Basis functions related to boundary conditions** </font> \\\n",
    "**`vel_BC_k_l_on_boundary.gmt`**, **`vel_BC_k_l_on_InSAR.gmt`** \\\n",
    "> **`<k = x, y, z & l = 001..024>`** \\\n",
    "> <font color=red> **Data vector made of 4 InSAR data + Boundary velocity** </font> \\\n",
    "> **`interpolated_A_new_ref_i.dat`** : *i* =1,2,3,4 (asc1, asc2, des1, des2) \\\n",
    "> **`<lon lat rate (cm/yr) Px Py Pz>`** \\\n",
    "> **`vel_InSAR_ref_no_refpoint.gmt`** : Boundary velocity data (in mm/yr) \\\n",
    ">\n",
    "> output files : \\\n",
    "> <font color=red> **predicted data** </font> \\\n",
    "> **`dLOS_Ai_predicted.dat`**, where *i* in 1 2 3 4 \\\n",
    "> **`vel_BC_on_boundary_pred.dat`** \n",
    "\n",
    "0. This code is a part of the joint inversion project (project4: joint inversion of GNSS and InSAR)\n",
    "1. This code uses two types of basis functions generated in the previous steps : BC and FT \n",
    "2. This code jointly invert 4 InSAR data and 1 boundary velocity data for coefficients of each basis function\n",
    "> (a) d=Gm (d=data; G=basis functions; m=model coefficients) \\\n",
    "> (b) m(best) = inv(G'G)G'd \\\n",
    "> (c) m(best) * continuous surface 3-D velocities (from basis functions) \n",
    "3. Try using different weighting factors for BC data and InSAR data.\n",
    "> BC velocity has smaller error than InSAR data in general. \\\n",
    "> Apply the weighting LSM to the final step only. \n",
    "4. All \"magic\" comments were removed due to errors when a converted script is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # If you use jupyter notebook, \n",
    "# # # comment out this cell and use the cell below instead. \n",
    "\n",
    "weight_for_BC = sys.argv[1]\n",
    "weight_for_BC = float(weight_for_BC)\n",
    "\n",
    "weight_for_InSAR = sys.argv[2]\n",
    "weight_for_InSAR = float(weight_for_InSAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # If you use the converted python script,\n",
    "# # # comment out this cell and use the cell above instead.\n",
    "\n",
    "# weight_for_BC = 1\n",
    "# weight_for_InSAR = 8 # This weighting factor will be inversely taken e.g., 8 => 1/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inversion types\n",
    "\n",
    "#inversion_flag = 1 # Simple LSM\n",
    "inversion_flag = 2 # Pseudo LSM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP1`: **BUILD a data vector,  $\\vec{d}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input files\n",
    "\n",
    "# 1. Boundary velocity (48 data points on the boundary)\n",
    "inputBC = \"vel_InSAR_ref_no_refpoint.gmt\"  #velocity boundary condition\n",
    "df_inputBC = pd.read_csv(inputBC, header = None, sep =' ')\n",
    "df_inputBC.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "df_inputBC.loc[:,['ve']] = df_inputBC.loc[:,['ve']]/10 # [mm/yr] to [cm/yr]  \n",
    "df_inputBC.loc[:,['vn']] = df_inputBC.loc[:,['vn']]/10 # [mm/yr] to [cm/yr]\n",
    "\n",
    "\n",
    "# 2. InSAR Ascending 1 (interpolated: sampled at 3621 regular knotpoints)\n",
    "inputInSAR1 = \"interpolated_A_new_ref_1.dat\"\n",
    "df_inputInSAR1 = pd.read_csv(inputInSAR1, header = None, sep = ' ')\n",
    "df_inputInSAR1.columns = ['lon','lat','velo','Px','Py','Pz']  #InSAR velo is in [cm/yr]\n",
    "\n",
    "# 3. InSAR Ascending 2 (interpolated: sampled at 3621 regular knotpoints)\n",
    "inputInSAR2 = \"interpolated_A_new_ref_2.dat\"\n",
    "df_inputInSAR2 = pd.read_csv(inputInSAR2, header = None, sep = ' ')\n",
    "df_inputInSAR2.columns = ['lon','lat','velo','Px','Py','Pz'] #InSAR velo is in [cm/yr]\n",
    "\n",
    "# 4. InSAR Descending 1 (interpolated: sampled at 3621 regular knotpoints)\n",
    "inputInSAR3 = \"interpolated_A_new_ref_3.dat\"\n",
    "df_inputInSAR3 = pd.read_csv(inputInSAR3, header = None, sep = ' ')\n",
    "df_inputInSAR3.columns = ['lon','lat','velo','Px','Py','Pz'] #InSAR velo is in [cm/yr]\n",
    "\n",
    "# 5. InSAR Descending 2 (interpolated: sampled at 3621 regular knotpoints)\n",
    "inputInSAR4 = \"interpolated_A_new_ref_4.dat\"\n",
    "df_inputInSAR4 = pd.read_csv(inputInSAR4, header = None, sep = ' ')\n",
    "df_inputInSAR4.columns = ['lon','lat','velo','Px','Py','Pz'] #InSAR velo is in [cm/yr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>NOTE: the pointing vectors are from the perspective of the ground! NOT of the satellite </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a boundary condition data vector along with coordinate information.\n",
    "# The x components are first and then the y components of the velocity.\n",
    "\n",
    "df_data_x = df_inputBC.iloc[:,[0,1,2]]  # saved vx data on the boudnary\n",
    "df_data_y = df_inputBC.iloc[:,[0,1,3]]  # saved vn data on the boundary\n",
    "\n",
    "df_data_x=df_data_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "df_data_y=df_data_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "# !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "# This step is very important to build the G matrix, G, which\n",
    "# has rows correspoding to the rows of the data vector, d, that have\n",
    "# the same coordinates!\n",
    "\n",
    "df_data_x=df_data_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_data_y=df_data_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# MERGE two columns (n*1) into a new column (2n*1)\n",
    "# > ignore_index = True : \n",
    "# >   have one continuous index numbers,\n",
    "# >     ignorning each of the two dfs original indices\n",
    "\n",
    "framesBC=[df_data_x,df_data_y]\n",
    "df_data_BC_all=pd.concat(framesBC,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "df_data_BC=df_data_BC_all.loc[:,['velo']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD a InSAR data vector along with coordinate information.\n",
    "# The rows of the InSAR data vector is in the order of Ascending 1, Asceding 2, Descending 1, and Descending 2. \n",
    "# Track the pointing vector values together with the rate data for the G-matrix.\n",
    "\n",
    "df_inputInSAR1=df_inputInSAR1.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR2=df_inputInSAR2.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR3=df_inputInSAR3.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "df_inputInSAR4=df_inputInSAR4.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "\n",
    "framesInSAR=[df_inputInSAR1,df_inputInSAR2,df_inputInSAR3,df_inputInSAR4]\n",
    "df_data_InSAR_all=pd.concat(framesInSAR,ignore_index=True) # merge the four dataFrames into one\n",
    "df_data_InSAR = df_data_InSAR_all.loc[:,['velo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the InSAR data vector and BC data vector into the final data vector\n",
    "\n",
    "framesFinal = [df_data_InSAR, df_data_BC]\n",
    "df_data_total = pd.concat(framesFinal,ignore_index=True) # merge the two dataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_total.columns = ['data'] # DATA VECTOR [InSAR1; InSAR2; InSAR3; InSAR4; BCx; BCy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:   \n",
    ">If your data have errors, \\\n",
    "build a **W** matrix in which its diagonal elements \\\n",
    "are of a vector made of 1/errors (in order).  \\\n",
    "Your new data vector **d<sub>w</sub>** = **W** **d**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STEP2`: **BUILD G-Matrix, $\\bar{\\bar{G}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on the read_csv(sep option) below: \\\n",
    "> **`'(?:,|\\s+)'`** - is a RegEx for selecting either comma or any number of consecutive spaces/tabs \\\n",
    "> See the answer at StackOverFlow [click here](https://stackoverflow.com/questions/43784422/pandas-error-when-reading-csv-file-using-sep-and-comment-arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-a : Build G matrix part only related to Boundary Condition on boundary points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many files do you have for the boudnary basis functions? :\n",
      "24 (fixed for now [9/29/2021])\n"
     ]
    }
   ],
   "source": [
    "df_G_BC_on_boundary = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "# Make a blank G matrix part related to Boundary Condition on boundary points\n",
    "\n",
    "# print('How many files do you have for the boudnary basis functions? :')\n",
    "# HowMany = input()\n",
    "# HowMany = int(HowMany)\n",
    "#24 for the boundary condition (7/28/2021)\n",
    "HowMany=24\n",
    "# print(\"24 (fixed for now [9/29/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_boundary.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_xrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_yrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gx (i)\n",
    "\n",
    "    df_xrot_x = df_xrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_xrot_y = df_xrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_xrot_x=df_xrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_xrot_y=df_xrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_xrot_x['flag']=np.array([1] * len(df_xrot_x)) # 1 = velocity east-component\n",
    "#     df_xrot_y['flag']=np.array([2] * len(df_xrot_y)) # 2 = velocity north-component\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_xrot_x=df_xrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_xrot_y=df_xrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gx=[df_xrot_x,df_xrot_y]\n",
    "    df_Gx=pd.concat(frames_Gx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gy (i)\n",
    "\n",
    "    df_yrot_x = df_yrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_yrot_y = df_yrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_yrot_x=df_yrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_yrot_y=df_yrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_yrot_x['flag']=np.array([1] * len(df_yrot_x)) # 1 = velocity east-component\n",
    "#     df_yrot_y['flag']=np.array([2] * len(df_yrot_y)) # 2 = velocity north-component\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_yrot_x=df_yrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot_y=df_yrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gy=[df_yrot_x,df_yrot_y]\n",
    "    df_Gy=pd.concat(frames_Gy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gz (i)\n",
    "\n",
    "    df_zrot_x = df_zrot.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_zrot_y = df_zrot.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_zrot_x=df_zrot_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_zrot_y=df_zrot_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "#     df_zrot_x['flag']=np.array([1] * len(df_zrot_x)) # 1 = velocity east-component\n",
    "#     df_zrot_y['flag']=np.array([2] * len(df_zrot_y)) # 2 = velocity north-component\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_zrot_x=df_zrot_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot_y=df_zrot_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gz=[df_zrot_x,df_zrot_y]\n",
    "    df_Gz=pd.concat(frames_Gz,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# SAVE G-matrix\n",
    "# Gmatrix = [Gxrot(1) Gyrot(1) Gzrot(1) ... Gxrot(HowMany) Gyrot(HowMany) Gzrot(HowMany)]\n",
    "    \n",
    "    df_G_BC_on_boundary[\"G_xrot\"+str(i)] = df_Gx.loc[:,['velo']]\n",
    "    df_G_BC_on_boundary[\"G_yrot\"+str(i)] = df_Gy.loc[:,['velo']]\n",
    "    df_G_BC_on_boundary[\"G_zrot\"+str(i)] = df_Gz.loc[:,['velo']]\n",
    "    \n",
    "#df_G_BC_on_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-b : Build G matrix part related to both Boundary Condition and Force Terms on boundary points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many grid cells do you have (where a set of 4 force terms defined) ? :\n",
      "140 (fixed for now [9/29/2021])\n"
     ]
    }
   ],
   "source": [
    "df_G_FT_on_boundary_eij = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "df_G_FT_on_boundary_ezz = pd.DataFrame(index = range(len(df_data_BC))) \n",
    "# Make a blank G matrix part related to Boundary Condition on boundary points\n",
    "\n",
    "# print('How many grid cells do you have (where a set of 4 force terms defined) ? :')\n",
    "HowMany=140\n",
    "# print(\"140 (fixed for now [9/29/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_boundary.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_boundary.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_boundary.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_hori_FT_\"+str(i)+\"_4\"+\"_on_boundary.gmt\" # z  horizontal\n",
    "    \n",
    "## READ files into two separate structures in the following orders:\n",
    "## 1st stru = {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany} \n",
    "## 2nd stru = {z1 .. zHowMany}\n",
    "##\n",
    "## And then merge these two structures into one in the order of ...\n",
    "##            {exx1, eyy1, exy1, ..., exxHowMany, eyyHowMany, exyHowMany, z1 .. zHowMany}\n",
    "\n",
    "\n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    df_exx.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','ve','vn','se','sn','corr']\n",
    "    \n",
    "# BUILD a column vector Gexx (i)\n",
    "\n",
    "    df_exx_x = df_exx.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_exx_y = df_exx.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_exx_x=df_exx_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exx_y=df_exx_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "    \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exx_x=df_exx_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exx_y=df_exx_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices   \n",
    "    frames_Gexx=[df_exx_x,df_exx_y]\n",
    "    df_Gexx=pd.concat(frames_Gexx,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "# BUILD a column vector Geyy (i)\n",
    "\n",
    "    df_eyy_x = df_eyy.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_eyy_y = df_eyy.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_eyy_x=df_eyy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_eyy_y=df_eyy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_eyy_x=df_eyy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy_y=df_eyy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Geyy=[df_eyy_x,df_eyy_y]\n",
    "    df_Geyy=pd.concat(frames_Geyy,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "# BUILD a column vector Gexy (i)\n",
    "\n",
    "    df_exy_x = df_exy.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_exy_y = df_exy.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_exy_x=df_exy_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_exy_y=df_exy_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_exy_x=df_exy_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy_y=df_exy_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gexy=[df_exy_x,df_exy_y]\n",
    "    df_Gexy=pd.concat(frames_Gexy,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "\n",
    "# BUILD a column vector G_ezz (i)\n",
    "\n",
    "    df_ezz_x = df_ezz.iloc[:,[0,1,2]]  # saved vx basis function on the boudnary\n",
    "    df_ezz_y = df_ezz.iloc[:,[0,1,3]]  # saved vn basis function on the boundary\n",
    "\n",
    "    df_ezz_x=df_ezz_x.rename(columns ={'ve': 'velo'}) #column name change\n",
    "    df_ezz_y=df_ezz_y.rename(columns ={'vn': 'velo'}) #column name change\n",
    "\n",
    "   \n",
    "    # !! SORT_VALUES !! # lat ascending first, and then lon ascending.\n",
    "    # This step is very important to build the G matrix, G, which\n",
    "    # has rows correspoding to the rows of the data vector, d, that have\n",
    "    # the same coordinates!\n",
    "    df_ezz_x=df_ezz_x.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz_y=df_ezz_y.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    \n",
    "    # MERGE two columns (n*1) into a new column (2n*1)\n",
    "    # > ignore_index = True : \n",
    "    # >   have one continuous index numbers,\n",
    "    # >     ignorning each of the two dfs original indices\n",
    "    frames_Gezz=[df_ezz_x,df_ezz_y]\n",
    "    df_Gezz=pd.concat(frames_Gezz,ignore_index=True) # merge the two dataFrames into one (vertically)\n",
    "    \n",
    "    \n",
    "    \n",
    "# SAVE a part of G-matrix (as in two different structures and then they will be merged later)\n",
    "\n",
    "    # 1st structure = [Gexx(1) Geyy(1) Gexy(1) ... Gexx(HowMany) Geyy(HowMany) Gexy(HowMany)]   \n",
    "    df_G_FT_on_boundary_eij[\"G_exx\"+str(i)] = df_Gexx.loc[:,['velo']]\n",
    "    df_G_FT_on_boundary_eij[\"G_eyy\"+str(i)] = df_Geyy.loc[:,['velo']]\n",
    "    df_G_FT_on_boundary_eij[\"G_exy\"+str(i)] = df_Gexy.loc[:,['velo']]\n",
    "\n",
    "    # 2nd structure = [Gezz(1) Gezz(2) ... Gezz(HowMany)] \n",
    "    df_G_FT_on_boundary_ezz[\"G_ezz\"+str(i)] = df_Gezz.loc[:,['velo']]\n",
    "    \n",
    "    \n",
    "# Merge the two structures horizontally !\n",
    "\n",
    "frames_Geij_Gezz = [df_G_FT_on_boundary_eij, df_G_FT_on_boundary_ezz]\n",
    "df_G_FT_on_boundary=pd.concat(frames_Geij_Gezz, axis=1) # merge the two dataFrames into one\n",
    "#df_G_FT_on_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FT and BC basis functions horizontally !\n",
    "\n",
    "frames_FT_BC=[df_G_FT_on_boundary, df_G_BC_on_boundary]\n",
    "df_G_FT_BC_on_boundary = pd.concat(frames_FT_BC, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)\n",
    "#df_G_FT_BC_on_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-c : Build G matrix part only related to Force Terms on InSAR data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many grid cells do you have (where a set of 4 force terms defined) ? :\n",
      "140 (fixed for now [9/29/2021])\n"
     ]
    }
   ],
   "source": [
    "df_G_FT_on_InSAR_hori = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "df_G_FT_on_InSAR_vert = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "# Make a blank G matrix part related to Force Terms on InSAR data points\n",
    "\n",
    "# df_G_FT_on_InSAR :\n",
    "# will be 'concat'ed with a frame of [df_G_FT_on_InSAR_hori, df_G_FT_on_InSAR_vert] and with axis=1\n",
    "\n",
    "# df_data_InSAR : velocity only\n",
    "# df_data_InSAR_all : lon lat vel px py pz\n",
    "\n",
    "df_px = df_data_InSAR_all.iloc[:,[3]]\n",
    "df_py = df_data_InSAR_all.iloc[:,[4]]\n",
    "df_pz = df_data_InSAR_all.iloc[:,[5]]\n",
    "\n",
    "#print('How many grid cells do you have (where a set of 4 force terms defined) ? :'\n",
    "HowMany=140\n",
    "#print(\"140 (fixed for now [9/29/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_exx = \"vel_hori_FT_\"+str(i)+\"_1\"+\"_on_InSAR.gmt\" #exx horizontal\n",
    "    inputfile_eyy = \"vel_hori_FT_\"+str(i)+\"_2\"+\"_on_InSAR.gmt\" #eyy horizontal\n",
    "    inputfile_exy = \"vel_hori_FT_\"+str(i)+\"_3\"+\"_on_InSAR.gmt\" #exy horizontal \n",
    "    inputfile_ezz = \"vel_vert_FT_\"+str(i)+\"_4\"+\"_on_InSAR.gmt\" # z  vertical\n",
    "    \n",
    "## READ files into two separate structures in the following orders:\n",
    "## 1st stru = {exx1_x*px + exx1_y*py, eyy1_x*px + eyy1_y*py , exy1_x*px + exy1_y*px, ..., \n",
    "##             eyyHowMany_x*px + eyyHowMany_y*py, exyHowMany_x*px + exyHowMany_y*py} \n",
    "## 2nd stru = {ezz1_z*pz, ezz2_z*pz, ..., ezzHowMany_z*pz}\n",
    "##\n",
    "## And then merge these two structures into one in the order of ...\n",
    "##            {exx1_x*px + exx1_y*py,...,exyHowMany_x*px + exyHowMany_y*py,ezz1_z*pz, ..., ezzHowMany_z*pz}\n",
    "\n",
    "\n",
    "    df_exx=pd.read_csv(inputfile_exx ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_eyy=pd.read_csv(inputfile_eyy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_exy=pd.read_csv(inputfile_exy ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')   \n",
    "    df_ezz=pd.read_csv(inputfile_ezz, header=None, sep=r'(?:,|\\s+)',\n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "\n",
    "    # ve = Px\n",
    "    # vn = Py\n",
    "    # vz = Pz \n",
    "    # This is because later on ...\n",
    "    # 've' will be multiplied by a dataFrame named 'Px'\n",
    "    # 'vn' will be multiplied by a dataFrame named 'Py'\n",
    "    # 'vz' will be multiplied by a dataFrame named 'Pz'\n",
    "    # pandas does Not allow to multiply (element wise) two different df in different names\n",
    "    \n",
    "    df_exx.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_eyy.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_exy.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_ezz.columns = ['lon','lat','Pz']\n",
    "\n",
    "# SORT THEM by the same order of the InSAR data,\n",
    "# which was already sorted in the beginning of this code.\n",
    "    \n",
    "    \n",
    "    df_exx=df_exx.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_eyy=df_eyy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_exy=df_exy.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_ezz=df_ezz.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# STACK the data frames of the basis function responses vertically!\n",
    "# InSAR data vector made of 4 separate pointing vectors\n",
    "# but the 4 data sets are defined in the same coordinates. \n",
    "# One needs to stack 4 of df_eij vertically. \n",
    "# len(df_eij) = len(InSAR_data)/4 \n",
    "    \n",
    "    \n",
    "    frames_exx_stack = [df_exx,df_exx,df_exx,df_exx]\n",
    "    df_exx_stacked=pd.concat(frames_exx_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    frames_eyy_stack = [df_eyy,df_eyy,df_eyy,df_eyy]\n",
    "    df_eyy_stacked=pd.concat(frames_eyy_stack,ignore_index=True) # merge the two dataFrames into one    \n",
    "\n",
    "    frames_exy_stack = [df_exy,df_exy,df_exy,df_exy]\n",
    "    df_exy_stacked=pd.concat(frames_exy_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    frames_ezz_stack = [df_ezz,df_ezz,df_ezz,df_ezz]\n",
    "    df_ezz_stacked=pd.concat(frames_ezz_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    \n",
    "# # BUILD a column vector Gx (i)\n",
    "    df_LOS_Gexx_x = df_exx_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gexx_y = df_exx_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gexx_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gexx = df_LOS_Gexx_x + df_LOS_Gexx_y\n",
    "    \n",
    "    \n",
    "    df_LOS_Geyy_x = df_eyy_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Geyy_y = df_eyy_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Geyy_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Geyy = df_LOS_Geyy_x + df_LOS_Geyy_y\n",
    "    \n",
    "    \n",
    "    df_LOS_Gexy_x = df_exy_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gexy_y = df_exy_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gexy_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gexy = df_LOS_Gexy_x + df_LOS_Gexy_y\n",
    "\n",
    "    \n",
    "    df_LOS_Gezz = df_ezz_stacked.loc[:,['Pz']] * df_pz\n",
    "    \n",
    "\n",
    "# SAVE G-matrix\n",
    "# df_G_FT_on_InSAR_hori = [(exx1_x*px + exx1_y*py), ... , (exyHowMany_x*px + exyHowMany_y*py)]\n",
    "\n",
    "    \n",
    "    df_G_FT_on_InSAR_hori[\"GexxXPx+GexxYPy \"+str(i)] = df_LOS_Gexx.loc[:,['Py']]\n",
    "    df_G_FT_on_InSAR_hori[\"GeyyXPx+GeyyYPy \"+str(i)] = df_LOS_Geyy.loc[:,['Py']]\n",
    "    df_G_FT_on_InSAR_hori[\"GexyXPx+GexyYPy \"+str(i)] = df_LOS_Gexy.loc[:,['Py']]\n",
    "    \n",
    "    df_G_FT_on_InSAR_vert[\"GezzZPz \"+str(i)] = df_LOS_Gezz['Pz']\n",
    "    \n",
    "    \n",
    "frames_FT_InSAR=[df_G_FT_on_InSAR_hori, df_G_FT_on_InSAR_vert]\n",
    "df_G_FT_on_InSAR = pd.concat(frames_FT_InSAR, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STEP 2-d : Build G matrix part related to both Force Terms and Boundary Conditions on InSAR data points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many files do you have for the boudnary basis functions? :\n",
      "24 (fixed for now [9/29/2021])\n"
     ]
    }
   ],
   "source": [
    "df_G_BC_on_InSAR = pd.DataFrame(index = range(len(df_data_InSAR))) \n",
    "# Make a blank G matrix part related to Bounndary Condition on InSAR data points\n",
    "\n",
    "# df_G_BC_on_InSAR :\n",
    "\n",
    "# df_data_InSAR : velocity only\n",
    "# df_data_InSAR_all : lon lat vel px py pz\n",
    "\n",
    "df_px = df_data_InSAR_all.iloc[:,[3]]\n",
    "df_py = df_data_InSAR_all.iloc[:,[4]]\n",
    "df_pz = df_data_InSAR_all.iloc[:,[5]]\n",
    "\n",
    "#print('How many files do you have for the boudnary basis functions? :')\n",
    "#24 for the boundary condition (7/28/2021)\n",
    "HowMany=24\n",
    "#print(\"24 (fixed for now [9/29/2021])\")\n",
    "\n",
    "for i in range(1,HowMany+1): \n",
    "\n",
    "    inputfile_xrot = \"vel_BC_x_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # x-rot \n",
    "    inputfile_yrot = \"vel_BC_y_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # y-rot \n",
    "    inputfile_zrot = \"vel_BC_z_\"+str(f\"{i:03}\")+\"_on_InSAR.gmt\" # z-rot \n",
    "\n",
    "# READ files in order {xrot1, yrot1, zrot1, ..., xrotHowMany, yrotHowMany, zrotHowMany}\n",
    "\n",
    "    df_xrot=pd.read_csv(inputfile_xrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_yrot=pd.read_csv(inputfile_yrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "    df_zrot=pd.read_csv(inputfile_zrot ,header=None, sep=r'(?:,|\\s+)', \n",
    "                           comment='#', engine='python')\n",
    "\n",
    "# CHANGE the column names \n",
    "    # ve = Px\n",
    "    # vn = Py\n",
    "    # vz = Pz \n",
    "    # This is because later on ...\n",
    "    # 've' will be multiplied by a dataFrame named 'Px'\n",
    "    # 'vn' will be multiplied by a dataFrame named 'Py'\n",
    "    # 'vz' will be multiplied by a dataFrame named 'Pz'\n",
    "    # pandas does Not allow to multiply (element wise) two different df in different names\n",
    "\n",
    "    df_xrot.columns = ['lon','lat','Px','Py','se','sn','corr']  \n",
    "    df_yrot.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    df_zrot.columns = ['lon','lat','Px','Py','se','sn','corr']\n",
    "    \n",
    "\n",
    "    \n",
    "# SORT THEM by the same order of the InSAR data,\n",
    "# which was already sorted in the beginning of this code.\n",
    "\n",
    "    df_xrot=df_xrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_yrot=df_yrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "    df_zrot=df_zrot.sort_values(['lat', 'lon'], ascending=[True, True])\n",
    "\n",
    "# STACK the data frames of the basis function responses vertically!\n",
    "# InSAR data vector made of 4 separate pointing vectors\n",
    "# but the 4 data sets are defined in the same coordinates. \n",
    "# One needs to stack 4 of df_irot vertically. \n",
    "# len(df_xrot) = len(InSAR_data)/4 \n",
    "    \n",
    "    \n",
    "    frames_xrot_stack = [df_xrot,df_xrot,df_xrot,df_xrot]\n",
    "    df_xrot_stacked=pd.concat(frames_xrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "\n",
    "    \n",
    "    frames_yrot_stack = [df_yrot,df_yrot,df_yrot,df_yrot]\n",
    "    df_yrot_stacked=pd.concat(frames_yrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "    frames_zrot_stack = [df_zrot,df_zrot,df_zrot,df_zrot]\n",
    "    df_zrot_stacked=pd.concat(frames_zrot_stack,ignore_index=True) # merge the two dataFrames into one\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # BUILD a column vector Gx (i)\n",
    "    df_LOS_Gxrot_x = df_xrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gxrot_y = df_xrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gxrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gxrot = df_LOS_Gxrot_x + df_LOS_Gxrot_y\n",
    "    \n",
    "\n",
    "    df_LOS_Gyrot_x = df_yrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gyrot_y = df_yrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gyrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gyrot = df_LOS_Gyrot_x + df_LOS_Gyrot_y\n",
    "    \n",
    "\n",
    "    df_LOS_Gzrot_x = df_zrot_stacked.loc[:,['Px']] * df_px \n",
    "    df_LOS_Gzrot_y = df_zrot_stacked.loc[:,['Py']] * df_py\n",
    "    df_LOS_Gzrot_x.columns=['Py'] # pandas doesn't add columns with different names directly\n",
    "    df_LOS_Gzrot = df_LOS_Gzrot_x + df_LOS_Gzrot_y\n",
    "    \n",
    "\n",
    "# SAVE G-matrix\n",
    "# df_G_FT_on_InSAR_hori = [(xrot1_x*px + xrot1_y*py), ... , (zrotHowMany_x*px + zrotHowMany_y*py)]\n",
    "\n",
    "    \n",
    "    df_G_BC_on_InSAR[\"GxrotXPx+GxrotYPy \"+str(i)] = df_LOS_Gxrot.loc[:,['Py']]\n",
    "    df_G_BC_on_InSAR[\"GyrotXPx+GyrotYPy \"+str(i)] = df_LOS_Gyrot.loc[:,['Py']]\n",
    "    df_G_BC_on_InSAR[\"GzrotXPx+GzrotYPy \"+str(i)] = df_LOS_Gzrot.loc[:,['Py']]\n",
    "    \n",
    "\n",
    "#df_G_BC_on_InSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FT and BC basis functions on InSAR data point horizontally !\n",
    "\n",
    "frames_FT_BC_on_InSAR=[df_G_FT_on_InSAR, df_G_BC_on_InSAR]\n",
    "df_G_FT_BC_on_InSAR = pd.concat(frames_FT_BC_on_InSAR, axis=1) #merge the two dataFrames into onee horizontally (axis = 1)\n",
    "#df_G_FT_BC_on_InSAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `STEP 2-FINAL : Build the complete G matrix `\n",
    "    \n",
    "### G-matrix = [df_G_FT_BC_on_InSAR; df_G_FT_BC_on_boundary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_FT_BC_on_boundary.columns = df_G_FT_BC_on_InSAR.columns\n",
    "# change column names of the df_G_FT_BC_on_boundary\n",
    "\n",
    "frames_FT_BC_final=[df_G_FT_BC_on_InSAR, df_G_FT_BC_on_boundary]\n",
    "df_G_final = pd.concat(frames_FT_BC_final, ignore_index=True) #merge the two dataFrames into one vertically (axis = 1)\n",
    "#df_G_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_data_total)!=len(df_G_final):\n",
    "    print(\"WARNING: Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `**STEP3**` : Joint Inversion of InSAR data and Boundary velocity \n",
    "> G-matrix = **df_G_final** \\\n",
    "> data vec = **df_data_total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Diagonal Weighting Matrix W\n",
    "\n",
    "nBC=len(df_data_BC)\n",
    "nInSAR=len(df_data_InSAR)\n",
    "nTotal=len(df_data_total)\n",
    "\n",
    "errorInSAR = np.ones(nInSAR)*weight_for_InSAR \n",
    "errorBC = np.ones(nBC)*weight_for_BC\n",
    "errorTotal = np.concatenate((errorInSAR, errorBC),axis=0)\n",
    "errorTotalinv = 1/errorTotal\n",
    "W = np.diag(errorTotalinv)\n",
    "\n",
    "# convert into a dataframe\n",
    "dfW = pd.DataFrame(W)\n",
    "\n",
    "# When calculating predictions, the non-weighted G-matrix is needed. \n",
    "# SAVE it!\n",
    "df_G_final_save = df_G_final\n",
    "\n",
    "# When calculating the misfit, the non-weighted data is needed. \n",
    "# SAVE it!\n",
    "df_data_total_save = df_data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the Diagonal Weighting Matrix dfW to the data vector and Gmatrix\n",
    "\n",
    "df_G_final = dfW @ df_G_final_save\n",
    "df_data_total = dfW @ df_data_total_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_prime = df_G_final.transpose() \n",
    "\n",
    "# G'G\n",
    "# >Two different ways to compute a matrix multiplication\n",
    "# >1st method\n",
    "GpG1=df_G_prime.dot(df_G_final) #G'G\n",
    "# >2nd method\n",
    "GpG2=df_G_prime @ df_G_final #G'G\n",
    "# >These results are same.\n",
    "# >Let's take the second one as G'G\n",
    "GpG = GpG2 #GpG is G'G\n",
    "# inv(G'G)\n",
    "# > Two different ways to obtain inverse matrix\n",
    "# > 1st method: np.linalg.inv \n",
    "df_inv_GpG = pd.DataFrame(np.linalg.inv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "# > 2nd method: np.linalg.pinv (Moore-Penrose inverse (SVD))\n",
    "df_pinv_GpG = pd.DataFrame(np.linalg.pinv(GpG.to_numpy()), GpG.columns, GpG.index)\n",
    "\n",
    "###############################\n",
    "## inv(G'G)*G'*d = model(LSM) #\n",
    "###############################\n",
    "if inversion_flag == 1:\n",
    "    df_model1=df_inv_GpG@df_G_prime@df_data_total #inversion\n",
    "else:\n",
    "    df_model1=df_pinv_GpG@df_G_prime@df_data_total #pseudo inversion\n",
    "\n",
    "## data predicted.\n",
    "#df_data_predicted = df_G_final@df_model1 \n",
    "df_data_predicted = df_G_final_save @ df_model1\n",
    "# df_G_final_save is the non-weighted G-matrix\n",
    "\n",
    "\n",
    "## norm2 misfit\n",
    "df_norm2=(df_data_total_save-df_data_predicted)**2\n",
    "df_norm2=df_norm2.to_numpy().sum()\n",
    "#df_norm2=np.lib.scimath.sqrt(df_norm2)\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "#  SAVE predicted InSAR velocity values on the InSAR data points in *.dat files   #\n",
    "# and SAVE predicted BC velocity values on the boundary data points in *.gmt file #\n",
    "###################################################################################\n",
    "\n",
    "#df_data_InSAR_all\n",
    "#df_data_BC_all\n",
    "\n",
    "# i in 1..4\n",
    "# dLOS_Ai_predicted.dat\n",
    "# vel_BC_on_boundary_pred.dat\n",
    "\n",
    "num_velo_point=len(df_data_InSAR_all)/4  # 4 data in a column vector\n",
    "num_velo_point=int(num_velo_point)\n",
    "\n",
    "num_velo_point2=len(df_data_BC_all)\n",
    "num_velo_point2=int(num_velo_point2)\n",
    "\n",
    "\n",
    "df_prediction_A1=df_data_predicted.iloc[0:num_velo_point] #dLOS Ascending 1 predicted\n",
    "\n",
    "df_prediction_A2=df_data_predicted.iloc[num_velo_point:2*num_velo_point] #dLOS Ascending 2 predicted\n",
    "df_prediction_A2=df_prediction_A2.reset_index(drop=True)\n",
    "# index starts from 0, and drop old index numbers\n",
    "\n",
    "df_prediction_A3=df_data_predicted.iloc[2*num_velo_point:3*num_velo_point] #dLOS Descending 1 predicted\n",
    "df_prediction_A3=df_prediction_A3.reset_index(drop=True)\n",
    "# index starts from 0, and drop old index numbers\n",
    "\n",
    "df_prediction_A4=df_data_predicted.iloc[3*num_velo_point:4*num_velo_point] #dLOS Descending 2 predicted\n",
    "df_prediction_A4=df_prediction_A4.reset_index(drop=True)\n",
    "# index starts from 0, and drop old index numbers\n",
    "\n",
    "\n",
    "#BC predicted x\n",
    "df_prediction_BC_x=df_data_predicted.iloc[4*num_velo_point:4*num_velo_point+int(num_velo_point2/2)] \n",
    "df_prediction_BC_x=df_prediction_BC_x.reset_index(drop=True)\n",
    "#BC predicted y\n",
    "df_prediction_BC_y=df_data_predicted.iloc[4*num_velo_point+int(num_velo_point2/2):4*num_velo_point+num_velo_point2] \n",
    "df_prediction_BC_y=df_prediction_BC_y.reset_index(drop=True)\n",
    "                   \n",
    "\n",
    "# Coordinate Template To Save Predicted InSAR Data\n",
    "df_InSAR_save = df_inputInSAR1.iloc[:,[0,1]]   \n",
    "# Coordinate Template To Save Predicted BC Data\n",
    "df_BC_save = df_data_BC_all.iloc[0:int(num_velo_point2/2),[0,1]]\n",
    "\n",
    "\n",
    "df_save_A1 = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A1['dLOS'] = df_prediction_A1 #append predicted dLOS\n",
    "\n",
    "df_save_A2 = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A2['dLOS'] = df_prediction_A2 #append predicted dLOS\n",
    "\n",
    "df_save_A3 = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A3['dLOS'] = df_prediction_A3 #append predicted dLOS\n",
    "\n",
    "df_save_A4 = df_InSAR_save.reset_index(drop=True)\n",
    "df_save_A4['dLOS'] = df_prediction_A4 #append predicted dLOS\n",
    "\n",
    "df_save_BC_XandY = df_BC_save.reset_index(drop=True)\n",
    "df_save_BC_XandY['vx'] = df_prediction_BC_x*10 # [cm/yr] to [mm/yr]\n",
    "df_save_BC_XandY['vn'] = df_prediction_BC_y*10 # [cm/yr] to [mm/yr]\n",
    "df_save_BC_XandY['se'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['sn'] = np.zeros(len(df_prediction_BC_y))\n",
    "df_save_BC_XandY['corr'] = np.zeros(len(df_prediction_BC_y))\n",
    "\n",
    "outputFILE_A1=\"dLOS_A1_predicted.dat\"\n",
    "df_save_A1.to_csv(outputFILE_A1, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_A2=\"dLOS_A2_predicted.dat\"\n",
    "df_save_A2.to_csv(outputFILE_A2, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_A3=\"dLOS_A3_predicted.dat\"\n",
    "df_save_A3.to_csv(outputFILE_A3, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_A4=\"dLOS_A4_predicted.dat\"\n",
    "df_save_A4.to_csv(outputFILE_A4, header=None, index=None, sep=' ',float_format='%g')\n",
    "\n",
    "outputFILE_BC_XandY=\"vel_BC_on_boundary_pred.dat\"\n",
    "df_save_BC_XandY.to_csv(outputFILE_BC_XandY, header=None, index=None, sep=' ', float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<div class=\"alert--icon\"> <i class=\"far fa-times-circle\"></i> </div>\n",
    "    <p> Simple LSM is not able to recover BC velocity well. Try weighted inversion </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <p> The weighted LSM works well. </p>\n",
    "    <b> There is tradeoff between data sets near the edges though. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-square statistics is : 1.000000 [cm/yr]**2 when weighting factor of InSAR was 1\n"
     ]
    }
   ],
   "source": [
    "print(\"chi-square statistics is : %f [cm/yr]**2 when the weighting factor for InSAR was %i\"  % (df_norm2, weight_for_InSAR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
